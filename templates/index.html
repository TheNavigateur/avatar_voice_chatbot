<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google ADK Voice Chatbot</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .container {
            background: rgba(255, 255, 255, 0.95);
            padding: 2rem;
            border-radius: 20px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
            text-align: center;
            max-width: 1000px;
            width: 90%;
            display: flex;
            flex-direction: column;
            max-height: 90vh;
            transition: all 0.3s ease;
        }

        .main-content {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            flex: 1;
            min-height: 0;
            width: 100%;
        }

        .left-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
        }

        .right-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            height: 100%;
            min-width: 300px;
        }

        h1 {
            color: #667eea;
            margin-bottom: 1.5rem;
            font-size: 2rem;
        }

        /* Three.js Avatar Container */
        .avatar-container {
            position: relative;
            width: 300px;
            height: 300px;
            margin: 0 auto 0 auto;
        }

        #avatar-canvas {
            width: 100%;
            height: 100%;
            border-radius: 20px;
            background: transparent;
        }

        #status {
            margin-bottom: 1rem;
            color: #666;
            font-style: italic;
            min-height: 1.5em;
            font-size: 0.95rem;
        }

        .mic-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 50%;
            width: 80px;
            height: 80px;
            font-size: 32px;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }

        .mic-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .mic-btn.listening {
            animation: pulse 1.5s infinite;
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5253 100%);
            box-shadow: 0 4px 15px rgba(238, 82, 83, 0.4);
        }

        .action-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 30px;
            padding: 12px 30px;
            font-size: 1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            margin: 10px;
            font-weight: 600;
        }

        .action-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .action-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(238, 82, 83, 0.7);
            }

            70% {
                transform: scale(1.1);
                box-shadow: 0 0 0 10px rgba(238, 82, 83, 0);
            }

            100% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(238, 82, 83, 0);
            }
        }

        /* Chat Layout */
        #transcript {
            margin-top: 0;
            padding: 1rem;
            border-radius: 15px;
            background: #f8f9fa;
            min-height: 200px;
            max-height: 500px;
            overflow-y: auto;
            /* Flex layout for bubbles */
            display: flex;
            flex-direction: column;
            gap: 10px;

            text-align: left;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.05);
            flex: 1;
            width: 100%;
            box-sizing: border-box;
        }

        .message {
            display: flex;
            flex-direction: column;
            max-width: 80%;
            animation: fadeIn 0.3s ease;
        }

        .user-message {
            align-self: flex-end;
            align-items: flex-end;
        }

        .bot-message {
            align-self: flex-start;
            align-items: flex-start;
        }

        .message-bubble {
            padding: 10px 15px;
            border-radius: 18px;
            position: relative;
            font-size: 0.95rem;
            line-height: 1.5;
            box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }

        .user-message .message-bubble {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .bot-message .message-bubble {
            background: #e4e6eb;
            color: #050505;
            border-bottom-left-radius: 4px;
        }

        .sender-name {
            font-size: 0.75rem;
            margin-bottom: 4px;
            opacity: 0.8;
            margin-left: 10px;
            /* Slight indent */
        }

        .user-message .sender-name {
            margin-right: 10px;
            margin-left: 0;
            color: #667eea;
            text-align: right;
            display: none;
            /* Hide 'You' label for cleaner look, or set to block if preferred */
        }

        .bot-message .sender-name {
            color: #666;
            /* display: block; */
            display: none;
            /* Let's hide both for now for pure bubble look, unless user wants them. Code below puts them inside bubble? No, separate. */
        }

        /* Let's actually put the name INSIDE the bubble for "similar layout" request if we want */
        /* Re-reading request: "Layout 'You' and 'Bot' in a similar way" */
        /* I'll use a structure where name is bold inside the bubble for now, to replicate old feel but better */

        .message-content-inner {
            display: flex;
            flex-direction: column;
        }

        .bubble-sender {
            font-weight: bold;
            font-size: 0.8em;
            margin-bottom: 2px;
            opacity: 0.9;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .hidden {
            display: none !important;
        }

        .subtitle {
            color: #666;
            margin-bottom: 2rem;
            font-size: 0.9rem;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            line-height: 1.6;
        }

        .interaction-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 80px;
            /* Fixed height taller than button */
            width: 100%;
            margin-top: 0;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .main-content {
                flex-direction: column;
            }

            .right-panel {
                width: 100%;
                min-width: auto;
                min-height: 300px;
            }

            .container {
                padding: 1.5rem;
                height: auto;
                max-height: none;
            }
        }
    </style>
    <!-- Import Maps -->
    <script type="importmap">
        {
            "imports": {
                "three": "https://unpkg.com/three@0.169.0/build/three.module.js",
                "three/addons/": "https://unpkg.com/three@0.169.0/examples/jsm/",
                "@pixiv/three-vrm": "https://unpkg.com/@pixiv/three-vrm@3.3.0/lib/three-vrm.module.js"
            }
        }
    </script>
</head>

<div class="container">
    <h1>Ask Jack about the RFAM database</h1>
    <p class="subtitle">By Naveen Chawla. Work in progress - natural language voice conversation with a custom 3D
        avatar with an SQL database, Google Search capable, integrated with three prominent text-to-speech voice AI
        model systems</p>

    <div class="main-content">
        <div class="left-panel">
            <div class="controls" style="margin-bottom: 0.5rem; text-align: center;">
                <label for="ttsProvider" style="margin-right: 0.5rem; color: #333;">Voice:</label>
                <select id="ttsProvider"
                    style="padding: 0.5rem; border-radius: 8px; border: 1px solid #ccc; margin-right: 1rem;">
                    <option value="google">Google Chirp</option>
                    <option value="openai" selected>OpenAI (Default)</option>
                    <option value="elevenlabs">ElevenLabs (Pro)</option>
                </select>

                <label for="avatarSelector" style="margin-right: 0.5rem; color: #333;">Avatar:</label>
                <select id="avatarSelector" style="padding: 0.5rem; border-radius: 8px; border: 1px solid #ccc;">
                    <option value="vrm_shadow" selected>Shadow (Anime)</option>
                    <option value="vrm_jack">Jack (Anime 1.0)</option>
                    <option value="glb">Ready Player Me</option>
                </select>
            </div>

            <!-- Three.js 3D Avatar -->
            <div class="avatar-container">
                <div id="avatar-loader"
                    style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: #667eea; font-weight: bold; background: rgba(255,255,255,0.8); padding: 5px 10px; border-radius: 8px; pointer-events: none;">
                    Loading Avatar...</div>
                <canvas id="avatar-canvas"></canvas>
            </div>

            <div class="interaction-container">
                <div id="status" class="hidden"></div>
                <button id="startBtn" class="action-btn start-btn">Start Conversation</button>
            </div>
        </div>

        <div class="right-panel">
            <div id="transcript"></div>
        </div>
    </div>
</div>

<!-- Three.js Avatar Setup (Module) -->
<script type="module">
    import * as THREE from 'three';
    import { GLTFLoader } from 'three/addons/loaders/GLTFLoader.js';
    import { VRMLoaderPlugin, VRMUtils } from '@pixiv/three-vrm';

    // Three.js Scene Setup
    let scene,
        camera,
        renderer,
        clock;

    // AVATAR STATE
    let currentAvatarType = 'vrm'; // 'vrm' or 'glb'
    let currentVrm = null; // The VRM instance
    let currentGlb = null; // The GLB instance
    let mixer = null; // For GLB Mixer (if needed, though we use manual morphs)
    let animationId = null;

    // GLB/RPM Legacy Variables
    window.facialMeshes = [];
    window.blinkState = {
        isBlinking: false,
        startTime: 0,
        duration: 150
    };

    // Map Emotions to ARKit Blend Shapes (for GLB)
    const expressionMap = {
        "Neutral": {},
        "Happy": { "mouthSmile": 0.7, "browInnerUp": 0.4, "cheekPuff": 0.2, "eyeSquintLeft": 0.3, "eyeSquintRight": 0.3 },
        "Sad": { "mouthFrownLeft": 0.5, "mouthFrownRight": 0.5, "browDownLeft": 0.4, "browDownRight": 0.4, "eyeLookDownLeft": 0.2, "eyeLookDownRight": 0.2 },
        "Surprised": { "eyeWideLeft": 0.6, "eyeWideRight": 0.6, "jawOpen": 0.1, "browInnerUp": 0.6, "browOuterUpLeft": 0.5, "browOuterUpRight": 0.5 },
        "Thinking": { "browDownLeft": 0.3, "browInnerUp": 0.3, "mouthPucker": 0.3, "eyeLookUpRight": 0.4, "eyeLookUpLeft": 0.4 },
        "Angry": { "browDownLeft": 0.8, "browDownRight": 0.8, "mouthShrugLower": 0.4, "eyeSquintLeft": 0.5, "eyeSquintRight": 0.5 },
        "Confused": { "browDownLeft": 0.5, "browOuterUpRight": 0.5, "mouthRollLower": 0.3, "eyeLookInLeft": 0.3 }
    };
    let currentEmotion = "Neutral";

    window.isSpeaking = false; // Expose globally for audio logic

    // Audio analysis
    window.audioContext = null;
    window.analyser = null;
    window.dataArray = null;
    window.currentAudioVolume = 0;
    window.audioElement = null;

    // VRM Blink State
    let blinkTimer = 0;
    let isBlinking = false;
    let blinkClose = false;

    const cameraYForVRM = 1.4;
    const cameraZForVRM = 1.2;

    function initThreeJS() {
        const canvas = document.getElementById('avatar-canvas');
        const container = document.querySelector('.avatar-container');

        // Scene
        scene = new THREE.Scene();

        // Camera - Standard Portrait
        camera = new THREE.PerspectiveCamera(30, container.clientWidth / container.clientHeight, 0.1, 20.0);
        camera.position.set(0, cameraYForVRM, cameraZForVRM);
        camera.lookAt(0, 1.35, 0);

        // Renderer
        renderer = new THREE.WebGLRenderer({
            canvas: canvas,
            alpha: true,
            antialias: true
        });
        renderer.setSize(container.clientWidth, container.clientHeight);
        renderer.setClearColor(0x000000, 0);
        renderer.outputEncoding = THREE.sRGBEncoding;
        renderer.toneMapping = THREE.ACESFilmicToneMapping;
        renderer.toneMappingExposure = 1.0;

        // Lights
        // Lights
        const hemiLight = new THREE.HemisphereLight(0xffffff, 0x444444);
        hemiLight.position.set(0, 20, 0);
        scene.add(hemiLight);

        const dirLight = new THREE.DirectionalLight(0xffffff);
        dirLight.position.set(3, 10, 10);
        dirLight.castShadow = true;
        scene.add(dirLight);

        // Strong frontal light from above
        const frontLight = new THREE.DirectionalLight(0xffffff, 2.0);
        frontLight.position.set(0, 10, 5); // Front-Top
        scene.add(frontLight);

        // Face Fill Light (Directly in front to light up cavities like mouth)
        const faceLight = new THREE.PointLight(0xffffff, 0.8);
        faceLight.position.set(0, 1.3, 2);
        scene.add(faceLight);

        // Strong ambient fill to reduce shadows
        const ambient = new THREE.AmbientLight(0xffffff, 1.2); // Slightly reduced to balance added lights
        scene.add(ambient);

        // Clock
        clock = new THREE.Clock();

        // Load Avatar
        loadCurrentAvatar();

        // Animation loop
        animate();
    }

    function loadCurrentAvatar() {
        const selector = document.getElementById('avatarSelector');
        const selectedValue = selector ? selector.value : 'vrm_shadow';

        if (selectedValue.startsWith('vrm')) {
            currentAvatarType = 'vrm';
            const file = selectedValue === 'vrm_jack' ? '/static/jack.vrm' : '/static/shadow.vrm';
            loadVRM(file);
        } else {
            currentAvatarType = 'glb';
            loadGLB();
        }
    }



    function loadVRM(modelUrl) {
        const loader = new GLTFLoader();
        loader.crossOrigin = 'anonymous';

        // Register VRM Loader Plugin
        loader.register((parser) => {
            return new VRMLoaderPlugin(parser);
        });

        const avatarLoader = document.getElementById('avatar-loader');

        if (avatarLoader) {
            avatarLoader.textContent = "Loading VRM...";
            avatarLoader.classList.remove('hidden');
        }
        console.log("Loading VRM:", modelUrl);

        loader.load(
            modelUrl,
            (gltf) => {
                const vrm = gltf.userData.vrm;

                if (currentVrm) {
                    scene.remove(currentVrm.scene);
                    VRMUtils.deepDispose(currentVrm.scene);
                }
                if (currentGlb) {
                    scene.remove(currentGlb);
                    // Dispose GLB...
                    currentGlb = null;
                }

                currentVrm = vrm;
                scene.add(vrm.scene);

                // Rotate if VRM 0.0 (old behavior often needed rotation)
                // VRM 1.0 (Jack) usually faces +Z, and our camera looks -Z?
                // Actually, if Jack faces away with Math.PI, he needs 0.
                if (modelUrl.includes('jack.vrm')) {
                    vrm.scene.rotation.y = 0;
                } else {
                    // Shadow (VRM 0.0) likely needs rotation
                    vrm.scene.rotation.y = Math.PI;
                }

                if (avatarLoader) avatarLoader.classList.add('hidden');

                // Adjust Camera for VRM (Portrait)
                // Use humanoid bone for head position if available
                const head = vrm.humanoid.getNormalizedBoneNode('head'); // 'head' in VRM 1.0 / 0.0 unified

                if (head) {
                    const worldPos = new THREE.Vector3();
                    head.getWorldPosition(worldPos);
                    const eyeY = worldPos.y + 0.05; // Slightly above center of head bone for eyes
                    camera.position.set(0, eyeY, worldPos.z + 0.7); // Adjust zoom
                    camera.lookAt(0, eyeY, 0);
                } else {
                    camera.position.set(0, cameraYForVRM, cameraZForVRM);
                    camera.lookAt(0, 1.35, 0);
                }

                console.log("VRM loaded:", vrm);
                // Start Unified Blink Loop
                startBlinking();
            },
            (progress) => {
                // console.log("Loading...", 100.0 * (progress.loaded / progress.total), "%");
            },
            (error) => { console.error("Failed to load VRM:", error); }
        );
    }

    let headBone = null; // Legacy GLB Head Bone

    function loadGLB() {
        const loader = new GLTFLoader();
        const avatarLoader = document.getElementById('avatar-loader');
        const modelUrl = '/static/avatar.glb';

        if (avatarLoader) {
            avatarLoader.textContent = "Loading RPM...";
            avatarLoader.classList.remove('hidden');
        }
        console.log("Loading GLB:", modelUrl);

        loader.load(modelUrl, (gltf) => {
            if (currentVrm) {
                scene.remove(currentVrm.scene);
                VRMUtils.deepDispose(currentVrm.scene);
                currentVrm = null;
            }
            if (currentGlb) {
                scene.remove(currentGlb);
            }

            currentGlb = gltf.scene;
            scene.add(currentGlb);

            // Setup GLB structure / meshes
            window.facialMeshes = [];
            headBone = null; // Reset head bone

            currentGlb.traverse((child) => {
                // Legacy Bone Detection for Idle Animation
                if (child.isBone) {
                    const boneName = child.name.toLowerCase();
                    if (boneName.includes('head') || boneName.includes('neck_01')) headBone = child;
                }

                if (child.isMesh && child.morphTargetDictionary) {
                    const meshData = {
                        mesh: child,
                        blinkIndices: [],
                        mouthOpenIndex: -1,
                        jawOpenIndex: -1,
                        mouthFunnelIndex: -1,
                        lipLowerIndices: [],
                        lipUpperIndices: []
                    };

                    let foundSomething = false;

                    for (const key in child.morphTargetDictionary) {
                        const lowerKey = key.toLowerCase();
                        // 1. Blink
                        if (lowerKey.includes('blink') || (lowerKey.includes('eye') && lowerKey.includes('close'))) {
                            meshData.blinkIndices.push(child.morphTargetDictionary[key]);
                            foundSomething = true;
                        }
                        // 2. Mouth Open / Jaw Open
                        if (lowerKey.includes('mouthopen') || lowerKey.includes('mouth_open')) {
                            meshData.mouthOpenIndex = child.morphTargetDictionary[key];
                            foundSomething = true;
                        }
                        if (lowerKey.includes('jawopen') || lowerKey.includes('jaw_open')) {
                            meshData.jawOpenIndex = child.morphTargetDictionary[key];
                            foundSomething = true;
                        }
                        // 3. Mouth Funnel (for Ooo sound shape)
                        if (lowerKey.includes('mouthfunnel') || lowerKey.includes('mouth_funnel')) {
                            meshData.mouthFunnelIndex = child.morphTargetDictionary[key];
                            foundSomething = true;
                        }

                        // 4. Lip Controls (Teeth exposure)
                        if (lowerKey.includes('mouthlowerdown')) {
                            meshData.lipLowerIndices.push(child.morphTargetDictionary[key]);
                            foundSomething = true;
                        }
                        if (lowerKey.includes('mouthupperup')) {
                            meshData.lipUpperIndices.push(child.morphTargetDictionary[key]);
                            foundSomething = true;
                        }
                    }

                    if (foundSomething) {
                        console.log("Registered Facial Mesh:", child.name);
                        window.facialMeshes.push(meshData);
                    }
                }
            });
            console.log("GLB Loaded. Facial meshes found:", window.facialMeshes.length);

            // RPM GLB Settings
            const box = new THREE.Box3().setFromObject(currentGlb);
            const size = new THREE.Vector3();
            box.getSize(size);
            currentGlb.scale.set(1, 1, 1);
            currentGlb.position.set(0, -size.y + 0.15, 0); // specific RPM offset

            // Adjust Camera for RPM (Closer) - Legacy Settings
            camera.position.set(0, 0, 0.67);
            camera.lookAt(0, 0, 0);

            // Start Legacy Blink Loop
            startBlinking();

            if (avatarLoader) avatarLoader.classList.add('hidden');
        });
    }

    function animate() {
        requestAnimationFrame(animate);

        const delta = clock.getDelta();

        if (currentAvatarType === 'vrm' && currentVrm) {
            updateBlink(delta);
            updateLipSync(delta);
            currentVrm.update(delta);
        } else if (currentAvatarType === 'glb' && currentGlb) {
            updateGLBAnimation(delta);
        }

        renderer.render(scene, camera);
    }

    // --- Legacy Blink Logic (Unified) ---
    let blinkTimeout = null;

    function startBlinking() {
        // Run for BOTH avatar types
        // Random blink interval between 1 and 3.5 seconds (more frequent)
        const nextBlink = Math.random() * 2500 + 1000;
        if (blinkTimeout) clearTimeout(blinkTimeout);
        blinkTimeout = setTimeout(() => {
            triggerBlink();
            startBlinking();
        }, nextBlink);
    }

    function triggerBlink() {
        // Shared State Initialization
        if (window.blinkState) {
            window.blinkState.startTime = Date.now();
            window.blinkState.duration = 170 + Math.random() * 50;
            window.blinkState.isBlinking = true;
        }

        // VRM Specific Flag (for internal status)
        if (currentAvatarType === 'vrm' && currentVrm) {
            isBlinking = true;
        }
    }

    // --- GLB/RPM Animation Logic (Legacy Restore) ---
    function updateGLBAnimation(delta) {
        // 1. Blink Logic
        if (window.blinkState) {
            const bs = window.blinkState;

            if (bs.isBlinking) {
                const elapsed = Date.now() - bs.startTime;
                const progress = elapsed / bs.duration;
                let weight = 0;
                if (progress < 0.35) { // Close
                    const p = progress / 0.35;
                    weight = p * p;
                } else if (progress < 0.45) { // Hold
                    weight = 1;
                } else if (progress < 1.0) { // Open
                    const p = (progress - 0.45) / 0.55;
                    weight = (1 - p) * (1 - p);
                } else { // Done
                    bs.isBlinking = false;
                    weight = 0;
                }

                // Apply to meshes
                if (window.facialMeshes) {
                    window.facialMeshes.forEach(data => {
                        if (data.blinkIndices) {
                            data.blinkIndices.forEach(idx => {
                                data.mesh.morphTargetInfluences[idx] = weight;
                            });
                        }
                    });
                }
            }
        }

        // 2. Lip Sync & Expression Logic
        if (window.facialMeshes) {
            window.facialMeshes.forEach(meshData => {
                const mesh = meshData.mesh;

                // Reset influences slightly (decay)
                for (let i = 0; i < mesh.morphTargetInfluences.length; i++) {
                    mesh.morphTargetInfluences[i] = THREE.MathUtils.lerp(mesh.morphTargetInfluences[i], 0, 0.1);
                }

                // Apply Base Emotion
                if (expressionMap[currentEmotion]) {
                    const targetShapes = expressionMap[currentEmotion];
                    for (const [shapeName, targetCtx] of Object.entries(targetShapes)) {
                        // Conflict resolution logic: skip mouth shapes if speaking
                        if (window.isSpeaking && (shapeName.includes('mouth') || shapeName.includes('jaw'))) continue;

                        const idx = mesh.morphTargetDictionary[shapeName];
                        if (idx !== undefined) {
                            const current = mesh.morphTargetInfluences[idx];
                            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(current, targetCtx, 0.1);
                        }
                    }
                }

                // Apply Lip Sync
                if (window.isSpeaking) {
                    const vol = window.currentAudioVolume || 0;
                    // Volume multiplier
                    let val = Math.min(vol * 2.5, 1.2);
                    const effectFactor = 1.0;

                    // 1. Jaw Open (Structure)
                    if (meshData.jawOpenIndex >= 0) {
                        const current = mesh.morphTargetInfluences[meshData.jawOpenIndex];
                        mesh.morphTargetInfluences[meshData.jawOpenIndex] = THREE.MathUtils.lerp(current, val * 0.06, effectFactor);
                    }

                    // 2. Mouth Open (General)
                    if (meshData.mouthOpenIndex >= 0) {
                        const current = mesh.morphTargetInfluences[meshData.mouthOpenIndex];
                        mesh.morphTargetInfluences[meshData.mouthOpenIndex] = THREE.MathUtils.lerp(current, val * 0.15, effectFactor);
                    }

                    // 3. Lip Retractors (Show Teeth!)
                    const teethFactor = 0.5;
                    if (meshData.lipLowerIndices) {
                        meshData.lipLowerIndices.forEach(idx => {
                            const current = mesh.morphTargetInfluences[idx];
                            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(current, val * teethFactor, effectFactor);
                        });
                    }

                    // 4. Mouth Funnel
                    if (meshData.mouthFunnelIndex >= 0) {
                        const current = mesh.morphTargetInfluences[meshData.mouthFunnelIndex];
                        mesh.morphTargetInfluences[meshData.mouthFunnelIndex] = THREE.MathUtils.lerp(current, val * 0.4, effectFactor);
                    }
                }
            });
        }

        // 3. Idle Head Movement (Legacy)
        const time = Date.now() * 0.001;
        if (headBone) {
            headBone.rotation.y = Math.sin(time * 0.5) * 0.05;
            headBone.rotation.x = Math.sin(time * 0.3) * 0.02;
        }
    }

    function updateBlink(delta) {
        // VRM 1.0 uses .expressionManager, VRM 0.0 uses .expressionManager (unified in new loader)
        // or fallbacks. The new VRMLoaderPlugin normalizes this.
        if (!currentVrm || !currentVrm.expressionManager) return;

        const expressionManager = currentVrm.expressionManager;
        const blinkName = 'blink'; // Standard unified name

        // Use Shared Blink State
        if (window.blinkState && window.blinkState.isBlinking) {
            const bs = window.blinkState;
            const elapsed = Date.now() - bs.startTime;
            const progress = elapsed / bs.duration;

            let weight = 0;

            if (progress < 0.35) { // Close (Fast)
                const p = progress / 0.35;
                weight = p * p;
            } else if (progress < 0.45) { // Hold (Closed)
                weight = 1;
            } else if (progress < 1.0) { // Open (Slower)
                const p = (progress - 0.45) / 0.55;
                weight = (1 - p) * (1 - p);
            } else { // Done
                bs.isBlinking = false;
                weight = 0;
            }

            expressionManager.setValue(blinkName, weight);
        } else {
            // Keep specific blinking logic for idle?
            // expressionManager.setValue(blinkName, 0); // Autodecay handles usually, but good to ensure
        }

        // Update the manager
        expressionManager.update();
    }

    function updateLipSync(delta) {
        if (!currentVrm || !currentVrm.expressionManager) return;
        const expressionManager = currentVrm.expressionManager;

        // Unified vowel name 'aa' (works for both VRM 0.0 'A' and 1.0 'aa')
        const vowelName = 'aa';

        if (!window.isSpeaking) {
            expressionManager.setValue(vowelName, 0);
            expressionManager.update();
            return;
        }

        const vol = window.currentAudioVolume || 0;
        const targetVal = Math.min(1.0, vol * 3.0);

        const currentVal = expressionManager.getValue(vowelName);
        const newVal = THREE.MathUtils.lerp(currentVal, targetVal, 0.4);

        expressionManager.setValue(vowelName, newVal);
        expressionManager.update();
    }

    window.addEventListener('resize', () => {
        const container = document.querySelector('.avatar-container');
        camera.aspect = container.clientWidth / container.clientHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(container.clientWidth, container.clientHeight);
    });

    window.addEventListener('load', initThreeJS);

    // Avatar Selector
    const avatarSelector = document.getElementById('avatarSelector');
    if (avatarSelector) {
        avatarSelector.addEventListener('change', (e) => {
            loadCurrentAvatar();
        });
    }

</script>
<script type="module">
    // Main App Logic (Module Scope)

    // We need to attach functions to window to be accessible? 
    // Actually the button uses addEventListener, so module scope is fine for init.
    // BUT 'startConversationMode' and others referenced in global scope or HTML events? 
    // In strict module type, functionality should be attached via JS.

    const startBtn = document.getElementById('startBtn');
    const statusDiv = document.getElementById('status');
    const transcriptDiv = document.getElementById('transcript');

    let sessionId = null;
    let recognition;
    let isConversationActive = false;
    let isBotSpeaking = false;
    let isProcessing = false;

    if ('webkitSpeechRecognition' in window) {
        recognition = new webkitSpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
            statusDiv.textContent = "Listening... ðŸŸ¢";
            statusDiv.style.color = "#42d392";
        };

        recognition.onend = () => {
            if (isConversationActive && !isBotSpeaking && !isProcessing) {
                console.log('Recognition ended (silence), restarting loop...');
                try {
                    recognition.start();
                } catch (e) { }
            } else if (!isConversationActive) {
                statusDiv.textContent = "";
                statusDiv.style.color = "#666";
            }
        };

        recognition.onresult = async (event) => {
            const transcript = event.results[0][0].transcript;
            addMessage('You', transcript);
            isProcessing = true;
            statusDiv.textContent = "Thinking... ðŸ§ ";
            statusDiv.style.color = "#764ba2";

            try {
                const requestBody = { message: transcript };
                if (sessionId) requestBody.session_id = sessionId;

                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(requestBody)
                });

                const data = await response.json();
                console.log('Received data:', data);

                if (data.session_id) sessionId = data.session_id;

                let botResponse = data.response;
                let shouldEndConversation = false;

                if (botResponse.includes('[END_CONVERSATION]')) {
                    shouldEndConversation = true;
                    botResponse = botResponse.replace('[END_CONVERSATION]', '').trim();
                }

                // Strip Emotion Tags
                const expressionRegex = /\[Expression:\s*([a-zA-Z]+)\]/i;
                const match = botResponse.match(expressionRegex);
                if (match) {
                    botResponse = botResponse.replace(expressionRegex, '').trim();
                }

                // addMessage('Bot', botResponse);  <-- Moved to speak() callback
                await speak(botResponse, shouldEndConversation);

            } catch (error) {
                console.error('Error:', error);
                statusDiv.textContent = "Error communicating with server";
                isProcessing = false;
                if (isConversationActive) {
                    try { recognition.start(); } catch (e) { }
                }
            }
        };

        recognition.onerror = (event) => {
            if (event.error === 'no-speech' && isConversationActive && !isBotSpeaking) {
                statusDiv.textContent = "Still listening...";
            } else {
                statusDiv.textContent = "Error: " + event.error;
            }
        };
    } else {
        statusDiv.textContent = "Web Speech API not supported in this browser.";
        startBtn.disabled = true;
        // Maybe polyfill or different handling
    }

    function startConversationMode() {
        isConversationActive = true;
        startBtn.classList.add('hidden');
        statusDiv.classList.remove('hidden');
        statusDiv.textContent = "Starting...";
        try {
            recognition.start();
        } catch (e) {
            console.error("Could not start recognition:", e);
        }
    }

    function stopConversationMode() {
        isConversationActive = false;
        if (recognition) recognition.stop();
        if (window.audioElement) {
            window.audioElement.pause();
            window.audioElement.currentTime = 0;
        }
        if (window.speechSynthesis) window.speechSynthesis.cancel();
        stopSpeaking();

        startBtn.classList.remove('hidden');
        statusDiv.textContent = "";
        statusDiv.classList.add('hidden');
    }

    // Attach Event Listeners (Module Safe)
    if (startBtn) {
        startBtn.addEventListener('click', startConversationMode);
    }


    function addMessage(sender, text) {
        const div = document.createElement('div');
        const isUser = sender === 'You';
        div.className = 'message ' + (isUser ? 'user-message' : 'bot-message');
        div.innerHTML = `
            <div class="message-bubble">
                <div class="bubble-sender">${sender}</div>
                <div>${text}</div>
            </div>
        `;
        const tDiv = document.getElementById('transcript');
        tDiv.appendChild(div);
        tDiv.scrollTop = tDiv.scrollHeight;
    }

    async function speak(text, shouldEnd = false) {
        isBotSpeaking = true;
        if (recognition) recognition.stop();

        try {
            const response = await fetch('/tts', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    text: text,
                    language_code: 'en-GB',
                    voice_name: 'en-GB-Chirp3-HD-Algenib',
                    provider: document.getElementById('ttsProvider').value
                })
            });

            const data = await response.json();
            const audioBlob = base64ToBlob(data.audio, 'audio/mp3');
            const audioUrl = URL.createObjectURL(audioBlob);

            if (!window.audioElement) {
                window.audioElement = new Audio();
                window.audioContext = new (window.AudioContext || window.webkitAudioContext)();
                window.analyser = window.audioContext.createAnalyser();
                window.analyser.fftSize = 256;
                const bufferLength = window.analyser.frequencyBinCount;
                window.dataArray = new Uint8Array(bufferLength);
                const audioSource = window.audioContext.createMediaElementSource(window.audioElement);
                audioSource.connect(window.analyser);
                window.analyser.connect(window.audioContext.destination);
            }

            window.audioElement.src = audioUrl;

            window.audioElement.onplay = () => {
                startSpeaking();
                addMessage('Bot', text); // Delay text display until audio starts
                updateAudioVolumeGlobal();
                statusDiv.textContent = "Speaking... ðŸ—£ï¸";
                statusDiv.style.color = "#667eea";
            };

            window.audioElement.onended = () => {
                stopSpeaking();
                URL.revokeObjectURL(audioUrl);
                isBotSpeaking = false;
                isProcessing = false;

                if (isConversationActive && !shouldEnd) {
                    statusDiv.textContent = "Listening... ðŸŸ¢";
                    statusDiv.style.color = "#42d392";
                    try { recognition.start(); } catch (e) { }
                } else if (shouldEnd) {
                    stopConversationMode();
                }
            };

            await window.audioElement.play();

        } catch (error) {
            console.error('TTS Error:', error);
            // Fallback UI logic removed for brevity, assume fetch works or handle simply
            stopConversationMode();
        }
    }

    // Globals for Audio Animation
    function startSpeaking() { window.isSpeaking = true; }
    function stopSpeaking() {
        window.isSpeaking = false;
        window.currentAudioVolume = 0;
    }

    function updateAudioVolumeGlobal() {
        if (!window.isSpeaking || !window.analyser) return;
        window.analyser.getByteFrequencyData(window.dataArray);
        let sum = 0;
        for (let i = 0; i < window.dataArray.length; i++) sum += window.dataArray[i];
        window.currentAudioVolume = (sum / window.dataArray.length) / 255;

        if (window.isSpeaking) requestAnimationFrame(updateAudioVolumeGlobal);
    }

    function base64ToBlob(base64, mimeType) {
        const byteCharacters = atob(base64);
        const byteNumbers = new Array(byteCharacters.length);
        for (let i = 0; i < byteCharacters.length; i++) { byteNumbers[i] = byteCharacters.charCodeAt(i); }
        const byteArray = new Uint8Array(byteNumbers);
        return new Blob([byteArray], { type: mimeType });
    }
</script>

</body>

</html>