<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google ADK Voice Chatbot</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .container {
            background: rgba(255, 255, 255, 0.95);
            padding: 2rem;
            border-radius: 20px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
            text-align: center;
            max-width: 1000px;
            width: 90%;
            display: flex;
            flex-direction: column;
            max-height: 90vh;
            transition: all 0.3s ease;
        }

        .main-content {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            flex: 1;
            min-height: 0;
            width: 100%;
        }

        .left-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
        }

        .right-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            height: 100%;
            min-width: 300px;
        }

        h1 {
            color: #667eea;
            margin-bottom: 1.5rem;
            font-size: 2rem;
        }

        /* Three.js Avatar Container */
        .avatar-container {
            position: relative;
            width: 300px;
            height: 300px;
            margin: 0 auto 0 auto;
        }

        #avatar-canvas {
            width: 100%;
            height: 100%;
            border-radius: 20px;
            background: transparent;
        }

        #status {
            margin-bottom: 1rem;
            color: #666;
            font-style: italic;
            min-height: 1.5em;
            font-size: 0.95rem;
        }

        .mic-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 50%;
            width: 80px;
            height: 80px;
            font-size: 32px;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }

        .mic-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .mic-btn.listening {
            animation: pulse 1.5s infinite;
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5253 100%);
            box-shadow: 0 4px 15px rgba(238, 82, 83, 0.4);
        }

        .action-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 30px;
            padding: 12px 30px;
            font-size: 1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            margin: 10px;
            font-weight: 600;
        }

        .action-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .action-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(238, 82, 83, 0.7);
            }

            70% {
                transform: scale(1.1);
                box-shadow: 0 0 0 10px rgba(238, 82, 83, 0);
            }

            100% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(238, 82, 83, 0);
            }
        }

        /* Chat Layout */
        #transcript {
            margin-top: 0;
            padding: 1rem;
            border-radius: 15px;
            background: #f8f9fa;
            min-height: 200px;
            max-height: 500px;
            overflow-y: auto;
            /* Flex layout for bubbles */
            display: flex;
            flex-direction: column;
            gap: 10px;

            text-align: left;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.05);
            flex: 1;
            width: 100%;
            box-sizing: border-box;
        }

        .message {
            display: flex;
            flex-direction: column;
            max-width: 80%;
            animation: fadeIn 0.3s ease;
        }

        .user-message {
            align-self: flex-end;
            align-items: flex-end;
        }

        .bot-message {
            align-self: flex-start;
            align-items: flex-start;
        }

        .message-bubble {
            padding: 10px 15px;
            border-radius: 18px;
            position: relative;
            font-size: 0.95rem;
            line-height: 1.5;
            box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }

        .user-message .message-bubble {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .bot-message .message-bubble {
            background: #e4e6eb;
            color: #050505;
            border-bottom-left-radius: 4px;
        }

        .sender-name {
            font-size: 0.75rem;
            margin-bottom: 4px;
            opacity: 0.8;
            margin-left: 10px;
            /* Slight indent */
        }

        .user-message .sender-name {
            margin-right: 10px;
            margin-left: 0;
            color: #667eea;
            text-align: right;
            display: none;
            /* Hide 'You' label for cleaner look, or set to block if preferred */
        }

        .bot-message .sender-name {
            color: #666;
            /* display: block; */
            display: none;
            /* Let's hide both for now for pure bubble look, unless user wants them. Code below puts them inside bubble? No, separate. */
        }

        /* Let's actually put the name INSIDE the bubble for "similar layout" request if we want */
        /* Re-reading request: "Layout 'You' and 'Bot' in a similar way" */
        /* I'll use a structure where name is bold inside the bubble for now, to replicate old feel but better */

        .message-content-inner {
            display: flex;
            flex-direction: column;
        }

        .bubble-sender {
            font-weight: bold;
            font-size: 0.8em;
            margin-bottom: 2px;
            opacity: 0.9;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .hidden {
            display: none !important;
        }

        .subtitle {
            color: #666;
            margin-bottom: 2rem;
            font-size: 0.9rem;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            line-height: 1.6;
        }

        .interaction-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 80px;
            /* Fixed height taller than button */
            width: 100%;
            margin-top: 0;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .main-content {
                flex-direction: column;
            }

            .right-panel {
                width: 100%;
                min-width: auto;
                min-height: 300px;
            }

            .container {
                padding: 1.5rem;
                height: auto;
                max-height: none;
            }
        }
    </style>
</head>

<div class="container">
    <h1>Ask Jack about the RFAM database</h1>
    <p class="subtitle">By Naveen Chawla. Work in progress - natural language voice conversation with a custom 3D
        avatar with an SQL database, Google Search capable, integrated with three prominent text-to-speech voice AI
        model systems</p>

    <div class="main-content">
        <div class="left-panel">
            <div class="controls" style="margin-bottom: 0.5rem; text-align: center;">
                <label for="ttsProvider" style="margin-right: 0.5rem; color: #333;">Voice:</label>
                <select id="ttsProvider"
                    style="padding: 0.5rem; border-radius: 8px; border: 1px solid #ccc; margin-right: 1rem;">
                    <option value="google">Google Chirp</option>
                    <option value="openai" selected>OpenAI (Default)</option>
                    <option value="elevenlabs">ElevenLabs (Pro)</option>
                </select>

                <label for="avatarSelector" style="margin-right: 0.5rem; color: #333;">Avatar:</label>
                <select id="avatarSelector" style="padding: 0.5rem; border-radius: 8px; border: 1px solid #ccc;">
                    <option value="vrm" selected>VRoid (Anime)</option>
                    <option value="glb">Ready Player Me</option>
                </select>
            </div>

            <!-- Three.js 3D Avatar -->
            <!-- Three.js 3D Avatar -->
            <div class="avatar-container">
                <div id="avatar-loader"
                    style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: #667eea; font-weight: bold; background: rgba(255,255,255,0.8); padding: 5px 10px; border-radius: 8px; pointer-events: none;">
                    Loading Avatar...</div>
                <canvas id="avatar-canvas"></canvas>
            </div>

            <div class="interaction-container">
                <div id="status" class="hidden"></div>
                <button id="startBtn" class="action-btn start-btn">Start Conversation</button>
            </div>
        </div>

        <div class="right-panel">
            <div id="transcript"></div>
        </div>
    </div>
</div>

<!-- Three.js Library -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
<!-- GLTFLoader -->
<script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>
<!-- Three-VRM -->
<script src="https://unpkg.com/@pixiv/three-vrm@0.6.11/lib/three-vrm.min.js"></script>

<!-- Three.js Avatar Setup -->
<script>
    // Three.js Scene Setup
    let scene,
        camera,
        renderer,
        clock;

    // AVATAR STATE
    let currentAvatarType = 'vrm'; // 'vrm' or 'glb'
    let currentVrm = null; // The VRM instance
    let currentGlb = null; // The GLB instance
    let mixer = null; // For GLB Mixer (if needed, though we use manual morphs)
    let animationId = null;

    // GLB/RPM Legacy Variables
    window.facialMeshes = [];
    window.blinkState = {
        isBlinking: false,
        startTime: 0,
        duration: 150
    };

    // Map Emotions to ARKit Blend Shapes (for GLB)
    const expressionMap = {
        "Neutral": {},
        "Happy": { "mouthSmile": 0.7, "browInnerUp": 0.4, "cheekPuff": 0.2, "eyeSquintLeft": 0.3, "eyeSquintRight": 0.3 },
        "Sad": { "mouthFrownLeft": 0.5, "mouthFrownRight": 0.5, "browDownLeft": 0.4, "browDownRight": 0.4, "eyeLookDownLeft": 0.2, "eyeLookDownRight": 0.2 },
        "Surprised": { "eyeWideLeft": 0.6, "eyeWideRight": 0.6, "jawOpen": 0.1, "browInnerUp": 0.6, "browOuterUpLeft": 0.5, "browOuterUpRight": 0.5 },
        "Thinking": { "browDownLeft": 0.3, "browInnerUp": 0.3, "mouthPucker": 0.3, "eyeLookUpRight": 0.4, "eyeLookUpLeft": 0.4 },
        "Angry": { "browDownLeft": 0.8, "browDownRight": 0.8, "mouthShrugLower": 0.4, "eyeSquintLeft": 0.5, "eyeSquintRight": 0.5 },
        "Confused": { "browDownLeft": 0.5, "browOuterUpRight": 0.5, "mouthRollLower": 0.3, "eyeLookInLeft": 0.3 }
    };
    let currentEmotion = "Neutral";

    let isSpeaking = false;

    // Audio analysis
    let audioContext;
    let analyser;
    let dataArray;
    let currentAudioVolume = 0;
    let audioElement = null;

    // VRM Blink State
    let blinkTimer = 0;
    let isBlinking = false;
    let blinkClose = false;

    const cameraYForVRM = 1.4;
    const cameraZForVRM = 1.2;

    function initThreeJS() {
        const canvas = document.getElementById('avatar-canvas');
        const container = document.querySelector('.avatar-container');

        // Scene
        scene = new THREE.Scene();

        // Camera - Standard Portrait
        camera = new THREE.PerspectiveCamera(30, container.clientWidth / container.clientHeight, 0.1, 20.0);
        camera.position.set(0, cameraYForVRM, cameraZForVRM);
        camera.lookAt(0, 1.35, 0);

        // Renderer
        renderer = new THREE.WebGLRenderer({
            canvas: canvas,
            alpha: true,
            antialias: true
        });
        renderer.setSize(container.clientWidth, container.clientHeight);
        renderer.setClearColor(0x000000, 0);
        renderer.outputEncoding = THREE.sRGBEncoding;

        // Lights
        const light = new THREE.DirectionalLight(0xffffff);
        light.position.set(1.0, 1.0, 1.0).normalize();
        scene.add(light);

        const ambient = new THREE.AmbientLight(0xffffff, 0.8);
        scene.add(ambient);

        // Clock
        clock = new THREE.Clock();

        // Load Avatar
        loadCurrentAvatar();

        // Animation loop
        animate();
    }

    function loadCurrentAvatar() {
        if (currentAvatarType === 'vrm') {
            loadVRM();
        } else {
            loadGLB();
        }
    }

    function loadVRM() {
        const loader = new THREE.GLTFLoader();
        const avatarLoader = document.getElementById('avatar-loader');
        const modelUrl = '/static/shadow.vrm';

        if (avatarLoader) {
            avatarLoader.textContent = "Loading VRoid...";
            avatarLoader.classList.remove('hidden');
        }
        console.log("Loading VRM:", modelUrl);

        loader.crossOrigin = 'anonymous';
        loader.load(
            modelUrl,
            (gltf) => {
                THREE.VRM.from(gltf).then((vrm) => {
                    scene.add(vrm.scene);
                    currentVrm = vrm;

                    // Cleanup GLB if exists
                    if (currentGlb) { scene.remove(currentGlb); currentGlb = null; }

                    // Rotate 180 deg to face camera
                    vrm.scene.rotation.y = Math.PI;

                    if (avatarLoader) avatarLoader.classList.add('hidden');

                    // Adjust Camera for VRM (Portrait)
                    const head = vrm.humanoid.getBoneNode(THREE.VRMSchema.HumanoidBoneName.Head);
                    if (head) {
                        const worldPos = new THREE.Vector3();
                        head.getWorldPosition(worldPos);
                        const eyeY = worldPos.y + 0.08;
                        camera.position.set(0, eyeY, worldPos.z + 0.65);
                        camera.lookAt(0, eyeY, 0);
                    } else {
                        camera.position.set(0, cameraYForVRM, cameraZForVRM);
                        camera.lookAt(0, 1.35, 0);
                    }

                    // Start Unified Blink Loop
                    startBlinking();
                });
            },
            (progress) => { },
            (error) => { console.error("Failed to load VRM:", error); }
        );
    }

    let headBone = null; // Legacy GLB Head Bone

    function loadGLB() {
        const loader = new THREE.GLTFLoader();
        const avatarLoader = document.getElementById('avatar-loader');
        const modelUrl = '/static/avatar.glb';

        if (avatarLoader) {
            avatarLoader.textContent = "Loading RPM...";
            avatarLoader.classList.remove('hidden');
        }
        console.log("Loading GLB:", modelUrl);

        loader.load(modelUrl, (gltf) => {
            currentGlb = gltf.scene;
            scene.add(currentGlb);

            // Cleanup VRM if exists
            if (currentVrm) { scene.remove(currentVrm.scene); currentVrm = null; }

            // Setup GLB structure / meshes
            window.facialMeshes = [];
            headBone = null; // Reset head bone

            currentGlb.traverse((child) => {
                // Legacy Bone Detection for Idle Animation
                if (child.isBone) {
                    const boneName = child.name.toLowerCase();
                    if (boneName.includes('head') || boneName.includes('neck_01')) headBone = child;
                }

                if (child.isMesh && child.morphTargetDictionary) {
                    // console.log("Found mesh:", child.name, Object.keys(child.morphTargetDictionary));

                    const meshData = {
                        mesh: child,
                        blinkIndices: [],
                        mouthOpenIndex: -1,
                        jawOpenIndex: -1,
                        mouthFunnelIndex: -1,
                        lipLowerIndices: [],
                        lipUpperIndices: []
                    };

                    let foundSomething = false;

                    for (const key in child.morphTargetDictionary) {
                        const lowerKey = key.toLowerCase();
                        // 1. Blink
                        if (lowerKey.includes('blink') || (lowerKey.includes('eye') && lowerKey.includes('close'))) {
                            meshData.blinkIndices.push(child.morphTargetDictionary[key]);
                            foundSomething = true;
                        }
                        // 2. Mouth Open / Jaw Open
                        if (lowerKey.includes('mouthopen') || lowerKey.includes('mouth_open')) {
                            meshData.mouthOpenIndex = child.morphTargetDictionary[key];
                            foundSomething = true;
                        }
                        if (lowerKey.includes('jawopen') || lowerKey.includes('jaw_open')) {
                            meshData.jawOpenIndex = child.morphTargetDictionary[key];
                            foundSomething = true;
                        }
                        // 3. Mouth Funnel (for Ooo sound shape)
                        if (lowerKey.includes('mouthfunnel') || lowerKey.includes('mouth_funnel')) {
                            meshData.mouthFunnelIndex = child.morphTargetDictionary[key];
                            foundSomething = true;
                        }

                        // 4. Lip Controls (Teeth exposure)
                        if (lowerKey.includes('mouthlowerdown')) {
                            meshData.lipLowerIndices.push(child.morphTargetDictionary[key]);
                            foundSomething = true;
                        }
                        if (lowerKey.includes('mouthupperup')) {
                            meshData.lipUpperIndices.push(child.morphTargetDictionary[key]);
                            foundSomething = true;
                        }
                    }

                    if (foundSomething) {
                        console.log("Registered Facial Mesh:", child.name);
                        window.facialMeshes.push(meshData);
                    }
                }
            });
            console.log("GLB Loaded. Facial meshes found:", window.facialMeshes.length);

            // RPM GLB Settings
            const box = new THREE.Box3().setFromObject(currentGlb);
            const size = new THREE.Vector3();
            box.getSize(size);
            currentGlb.scale.set(1, 1, 1);
            currentGlb.position.set(0, -size.y + 0.15, 0); // specific RPM offset

            // Adjust Camera for RPM (Closer) - Legacy Settings
            camera.position.set(0, 0, 0.67);
            camera.lookAt(0, 0, 0);

            // Start Legacy Blink Loop
            startBlinking();

            if (avatarLoader) avatarLoader.classList.add('hidden');
        });
    }

    function animate() {
        requestAnimationFrame(animate);

        const delta = clock.getDelta();

        if (currentAvatarType === 'vrm' && currentVrm) {
            updateBlink(delta);
            updateLipSync(delta);
            currentVrm.update(delta);
        } else if (currentAvatarType === 'glb' && currentGlb) {
            updateGLBAnimation(delta);
        }

        renderer.render(scene, camera);
    }

    // --- Legacy Blink Logic (Unified) ---
    let blinkTimeout = null;

    function startBlinking() {
        // Run for BOTH avatar types
        // Random blink interval between 1 and 3.5 seconds (more frequent)
        const nextBlink = Math.random() * 2500 + 1000;
        if (blinkTimeout) clearTimeout(blinkTimeout);
        blinkTimeout = setTimeout(() => {
            triggerBlink();
            startBlinking();
        }, nextBlink);
    }

    function triggerBlink() {
        console.log("triggerBlink called. Avatar:", currentAvatarType);

        // Shared State Initialization
        if (window.blinkState) {
            window.blinkState.startTime = Date.now();
            window.blinkState.duration = 170 + Math.random() * 50;
            window.blinkState.isBlinking = true;
            console.log("Blink Triggered!", window.blinkState);
        }

        // VRM Specific Flag (Optional, but good for quick checks if needed, or remove if unused)
        if (currentAvatarType === 'vrm' && currentVrm) {
            isBlinking = true;
        }
        // GLB Blink Trigger
        if (currentAvatarType === 'glb' && (!window.facialMeshes || window.facialMeshes.length === 0)) return;
    }

    // --- GLB/RPM Animation Logic (Legacy Restore) ---
    function updateGLBAnimation(delta) {
        // 1. Blink Logic
        if (window.blinkState) {
            const bs = window.blinkState;
            // Trigger is handled by startBlinking() setTimeout loop now

            if (bs.isBlinking) {
                const elapsed = Date.now() - bs.startTime;
                const progress = elapsed / bs.duration;
                let weight = 0;
                if (progress < 0.35) { // Close
                    const p = progress / 0.35;
                    weight = p * p;
                } else if (progress < 0.45) { // Hold
                    weight = 1;
                } else if (progress < 1.0) { // Open
                    const p = (progress - 0.45) / 0.55;
                    weight = (1 - p) * (1 - p);
                } else { // Done
                    bs.isBlinking = false;
                    weight = 0;
                }

                // Apply to meshes
                if (window.facialMeshes) {
                    window.facialMeshes.forEach(data => {
                        if (data.blinkIndices) {
                            data.blinkIndices.forEach(idx => {
                                data.mesh.morphTargetInfluences[idx] = weight;
                            });
                        }
                    });
                }
            }
        }

        // 2. Lip Sync & Expression Logic
        if (window.facialMeshes) {
            window.facialMeshes.forEach(meshData => {
                const mesh = meshData.mesh;

                // Reset influences slightly (decay)
                for (let i = 0; i < mesh.morphTargetInfluences.length; i++) {
                    mesh.morphTargetInfluences[i] = THREE.MathUtils.lerp(mesh.morphTargetInfluences[i], 0, 0.1);
                }

                // Apply Base Emotion
                if (expressionMap[currentEmotion]) {
                    const targetShapes = expressionMap[currentEmotion];
                    for (const [shapeName, targetCtx] of Object.entries(targetShapes)) {
                        // Conflict resolution logic: skip mouth shapes if speaking
                        if (isSpeaking && (shapeName.includes('mouth') || shapeName.includes('jaw'))) continue;

                        const idx = mesh.morphTargetDictionary[shapeName];
                        if (idx !== undefined) {
                            const current = mesh.morphTargetInfluences[idx];
                            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(current, targetCtx, 0.1);
                        }
                    }
                }

                // Apply Lip Sync
                if (isSpeaking) {
                    // Volume multiplier
                    let val = Math.min(currentAudioVolume * 2.5, 1.2);

                    const effectFactor = 1.0;

                    // 1. Jaw Open (Structure)
                    if (meshData.jawOpenIndex >= 0) {
                        const current = mesh.morphTargetInfluences[meshData.jawOpenIndex];
                        // Reduced Jaw: 0.25 (was 0.5) - Subtle structure, relying on lips for teeth
                        mesh.morphTargetInfluences[meshData.jawOpenIndex] = THREE.MathUtils.lerp(current, val * 0.06, effectFactor);
                    }

                    // 2. Mouth Open (General)
                    if (meshData.mouthOpenIndex >= 0) {
                        const current = mesh.morphTargetInfluences[meshData.mouthOpenIndex];
                        mesh.morphTargetInfluences[meshData.mouthOpenIndex] = THREE.MathUtils.lerp(current, val * 0.15, effectFactor);
                    }

                    // 3. Lip Retractors (Show Teeth!)
                    const teethFactor = 0.5; // How much to pull lips back

                    if (meshData.lipLowerIndices && meshData.lipLowerIndices.length > 0) {
                        meshData.lipLowerIndices.forEach(idx => {
                            const current = mesh.morphTargetInfluences[idx];
                            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(current, val * teethFactor, effectFactor);
                        });
                    }
                    if (meshData.lipUpperIndices && meshData.lipUpperIndices.length > 0) {
                        meshData.lipUpperIndices.forEach(idx => {
                            const current = mesh.morphTargetInfluences[idx];
                            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(current, val * teethFactor * 0.0, effectFactor); // Upper moves less
                        });
                    }

                    // 4. Mouth Funnel (The "Middle" Fix)
                    // This pulls the corners in, making it look less "wide/strained"
                    if (meshData.mouthFunnelIndex >= 0) {
                        const current = mesh.morphTargetInfluences[meshData.mouthFunnelIndex];
                        // Apply moderate funneling to round out the shape
                        mesh.morphTargetInfluences[meshData.mouthFunnelIndex] = THREE.MathUtils.lerp(current, val * 0.4, effectFactor);
                    }
                }
            });
        }

        // 3. Idle Head Movement (Legacy)
        const time = Date.now() * 0.001;
        if (headBone) {
            headBone.rotation.y = Math.sin(time * 0.5) * 0.05;
            headBone.rotation.x = Math.sin(time * 0.3) * 0.02;
        }
    }

    function updateBlink(delta) {
        // Support for VRM 0.x (blendShapeProxy) vs 1.0+ (expressionManager)
        const manager = currentVrm.blendShapeProxy || currentVrm.expressionManager;
        if (!manager) return;

        const BlinkPreset = currentVrm.blendShapeProxy
            ? THREE.VRMSchema.BlendShapePresetName.Blink
            : THREE.VRMSchema.ExpressionPresetName.Blink;

        // Use Shared Blink State
        if (window.blinkState && window.blinkState.isBlinking) {
            // console.log("Updating VRM Blink...", window.blinkState.isBlinking);
            const bs = window.blinkState;
            const elapsed = Date.now() - bs.startTime;
            const progress = elapsed / bs.duration;

            let weight = 0;

            if (progress < 0.35) { // Close (Fast)
                const p = progress / 0.35;
                weight = p * p; // EaseIn
            } else if (progress < 0.45) { // Hold (Closed)
                weight = 1;
            } else if (progress < 1.0) { // Open (Slower)
                const p = (progress - 0.45) / 0.55;
                weight = (1 - p) * (1 - p); // EaseOut
            } else { // Done
                bs.isBlinking = false;
                // isBlinking = false; // Sync global just in case, though unused
                weight = 0;
            }

            manager.setValue(BlinkPreset, weight);
        } else {
            // Ensure it's closed if not blinking
            // manager.setValue(BlinkPreset, 0);
        }
    }

    function updateLipSync(delta) {
        // Support for VRM 0.x (blendShapeProxy) vs 1.0+ (expressionManager)
        const manager = currentVrm.blendShapeProxy || currentVrm.expressionManager;
        if (!manager) return;

        // Determine Presets based on version
        const isV0 = !!currentVrm.blendShapeProxy;
        const SchemaStart = isV0 ? THREE.VRMSchema.BlendShapePresetName : THREE.VRMSchema.ExpressionPresetName;

        if (!isSpeaking) {
            const vowels = ['Aa', 'Ih', 'Ou', 'Ee', 'Oh']; // Capitalized for both usually works or is mapped
            vowels.forEach(v => {
                const preset = SchemaStart[v] || SchemaStart[v.toLowerCase()] || SchemaStart[v.toUpperCase()];
                if (preset) {
                    manager.setValue(preset, 0);
                }
            });
            return;
        }

        // Map Audio Volume directly to "Aa" or "A"
        const pName = SchemaStart.Aa || SchemaStart.A;
        if (!pName) return;

        const targetVal = Math.min(1.0, currentAudioVolume * 3.0);

        const currentVal = manager.getValue(pName);
        const newVal = THREE.MathUtils.lerp(currentVal, targetVal, 0.4);

        manager.setValue(pName, newVal);
    }

    // --- Shared Audio Control Functions ---
    function startSpeaking() { isSpeaking = true; }

    function stopSpeaking() {
        isSpeaking = false;
        currentAudioVolume = 0;
        if (currentVrm) {
            const manager = currentVrm.blendShapeProxy || currentVrm.expressionManager;
            if (manager) {
                const isV0 = !!currentVrm.blendShapeProxy;
                const SchemaStart = isV0 ? THREE.VRMSchema.BlendShapePresetName : THREE.VRMSchema.ExpressionPresetName;
                const pName = SchemaStart.Aa || SchemaStart.A;
                if (pName) manager.setValue(pName, 0);
            }
        }
    }

    function updateAudioVolume() {
        if (!isSpeaking || !analyser) return;
        analyser.getByteFrequencyData(dataArray);
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) sum += dataArray[i];
        currentAudioVolume = (sum / dataArray.length) / 255;

        // DEBUG VOLUME
        if (Math.random() < 0.05) console.log("Audio Volume:", currentAudioVolume);

        if (isSpeaking) requestAnimationFrame(updateAudioVolume);
    }

    window.addEventListener('resize', () => {
        const container = document.querySelector('.avatar-container');
        camera.aspect = container.clientWidth / container.clientHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(container.clientWidth, container.clientHeight);
    });

    window.addEventListener('load', initThreeJS);

    // Avatar Selector
    const avatarSelector = document.getElementById('avatarSelector');
    if (avatarSelector) {
        avatarSelector.addEventListener('change', (e) => {
            currentAvatarType = e.target.value;
            loadCurrentAvatar();
        });
    }

</script>
<script>
    const startBtn = document.getElementById('startBtn');
    const statusDiv = document.getElementById('status');
    const transcriptDiv = document.getElementById('transcript');

    let sessionId = null;
    let recognition;
    let isConversationActive = false; // Flag for continuous mode
    let isBotSpeaking = false; // Flag to prevent self-triggering
    let isProcessing = false; // Flag to prevent restart during fetch

    if ('webkitSpeechRecognition' in window) {
        recognition = new webkitSpeechRecognition();
        recognition.continuous = false; // We manually restart
        recognition.interimResults = false;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
            statusDiv.textContent = "Listening... ðŸŸ¢";
            statusDiv.style.color = "#42d392"; // Greenish
        };

        recognition.onend = () => {
            // If conversation is active and we are NOT processing a result and NOT speaking
            // Then it was just a silence timeout or no-speech event. Restart listening.
            if (isConversationActive && !isBotSpeaking && !isProcessing) {
                console.log('Recognition ended (silence), restarting loop...');

                // Don't change status text to "Processing", keep "Listening" (or similar)
                // to avoid confusing the user.
                try {
                    recognition.start();
                } catch (e) {
                    // Ignore if already started
                }
            } else if (!isConversationActive) {
                // Only show "Processing" if we are truly stopping/done?
                // Actually if we are not active, we are just stopped.
                statusDiv.textContent = "";
                statusDiv.style.color = "#666";
            }
            // If isProcessing is true, do NOTHING here. Let the fetch/speak completion handle the restart.
        };

        recognition.onresult = async (event) => {
            const transcript = event.results[0][0].transcript;
            addMessage('You', transcript);

            // Set processing flag to true so onend doesn't restart listing immediately
            isProcessing = true;

            // Visual feedback for processing
            statusDiv.textContent = "Thinking... ðŸ§ ";
            statusDiv.style.color = "#764ba2"; // Purple

            try {
                const requestBody = {
                    message: transcript
                };

                if (sessionId) {
                    requestBody.session_id = sessionId;
                }

                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(requestBody)
                });


                const data = await response.json();
                console.log('Received data:', data);
                console.log('Response text:', data.response);

                if (data.session_id) {
                    sessionId = data.session_id;
                }

                let botResponse = data.response;
                console.log('Bot response (raw):', botResponse);

                // Check for termination token
                let shouldEndConversation = false;

                if (botResponse.includes('[END_CONVERSATION]')) {
                    shouldEndConversation = true;
                    botResponse = botResponse.replace('[END_CONVERSATION]', '').trim();
                }

                // --- EMOTION PARSING (SIMPLIFIED FOR VRM) ---
                // VRM usually handles emotions via Presets. 
                // We can map Emotions to VRM Presets here if desired.
                // For now, let's just strip the tag.
                const expressionRegex = /\[Expression:\s*([a-zA-Z]+)\]/i;
                const match = botResponse.match(expressionRegex);

                if (match && match[1]) {
                    let detectedEmotion = match[1];
                    console.log("Detected Emotion (Tag):", detectedEmotion);
                    // TODO: Map to VRM Expression if needed: currentVrm.expressionManager.setValue(detectedEmotion, 1.0);

                    // Remove tag from spoken text
                    botResponse = botResponse.replace(expressionRegex, '').trim();
                }

                addMessage('Bot', botResponse);

                // Respond
                await speak(botResponse, shouldEndConversation);

            } catch (error) {
                console.error('Error:', error);
                statusDiv.textContent = "Error communicating with server";
                isProcessing = false; // Reset if error

                // Force restart listening if error occurred
                if (isConversationActive) {
                    try {
                        recognition.start();
                    } catch (e) { }
                }
            }
        };


        recognition.onerror = (event) => {
            console.error('Speech recognition error', event.error);

            if (event.error === 'no-speech' && isConversationActive && !isBotSpeaking) {
                // Just restart if it timed out with no speech
                statusDiv.textContent = "Still listening...";
                // onend will likely handle the restart
            } else {
                statusDiv.textContent = "Error: " + event.error;
            }
        };
    } else {
        statusDiv.textContent = "Web Speech API not supported in this browser.";
        startBtn.disabled = true;
    }

    function startConversationMode() {
        isConversationActive = true;

        // UI Updates
        startBtn.classList.add('hidden');
        statusDiv.classList.remove('hidden'); // Show status
        statusDiv.textContent = "Starting...";

        // Start listening immediately
        try {
            recognition.start();
        } catch (e) {
            console.error("Could not start recognition:", e);
            statusDiv.textContent = "Error starting listening. Please try again.";
        }
    }

    function stopConversationMode() {
        isConversationActive = false;

        if (recognition) {
            recognition.stop();
        }

        if (audioElement) {
            audioElement.pause();
            audioElement.currentTime = 0;
        }

        if (window.speechSynthesis) {
            window.speechSynthesis.cancel();
        }

        stopSpeaking(); // Avatar animation stop

        // UI Updates
        startBtn.classList.remove('hidden');
        statusDiv.textContent = "";
        statusDiv.classList.add('hidden'); // Hide status
    }

    startBtn.addEventListener('click', startConversationMode);


    function addMessage(sender, text) {
        const div = document.createElement('div');
        const isUser = sender === 'You';
        div.className = 'message ' + (isUser ? 'user-message' : 'bot-message');

        // New Layout: Bubble with sender inside
        div.innerHTML = `
            <div class="message-bubble">
                <div class="bubble-sender">${sender}</div>
                <div>${text}</div>
            </div>
        `;

        const transcriptDiv = document.getElementById('transcript'); // Ensure transcriptDiv is defined in scope or use global
        const tDiv = document.getElementById('transcript');
        tDiv.appendChild(div);
        tDiv.scrollTop = tDiv.scrollHeight;
    }

    async function speak(text, shouldEnd = false) {
        isBotSpeaking = true;

        // Stop listening while speaking to prevent feedback loop
        if (recognition) {
            recognition.stop();
        }

        try {

            // Call the Google TTS endpoint
            const response = await fetch('/tts', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    text: text,
                    language_code: 'en-GB',
                    voice_name: 'en-GB-Chirp3-HD-Algenib',
                    provider: document.getElementById('ttsProvider').value
                })
            });

            const data = await response.json();

            // Convert base64 audio to blob and play
            const audioBlob = base64ToBlob(data.audio, 'audio/mp3');
            const audioUrl = URL.createObjectURL(audioBlob);

            // Setup Web Audio API and audio element (only once)
            if (!audioElement) {
                audioElement = new Audio();

                // Create AudioContext and analyser
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                const bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);

                // Create source and connect (only once!)
                audioSource = audioContext.createMediaElementSource(audioElement);
                audioSource.connect(analyser);
                analyser.connect(audioContext.destination);
            }

            // Set new audio source and play
            audioElement.src = audioUrl;

            // Start Three.js avatar animation when audio starts playing
            audioElement.onplay = () => {
                startSpeaking();
                updateAudioVolume();
                statusDiv.textContent = "Speaking... ðŸ—£ï¸";
                statusDiv.style.color = "#667eea"; // Blue
            };

            // Stop Three.js avatar animation when audio ends
            audioElement.onended = () => {
                stopSpeaking();
                URL.revokeObjectURL(audioUrl);

                isBotSpeaking = false;
                isProcessing = false; // Done processing/speaking

                // RESTART LISTENING HERE if conversation is active AND not ending
                if (isConversationActive && !shouldEnd) {
                    statusDiv.textContent = "Listening... ðŸŸ¢";
                    statusDiv.style.color = "#42d392";

                    try {
                        recognition.start();
                    } catch (e) {
                        console.log('Already started');
                    }
                } else if (shouldEnd) {
                    // End conversation properly AFTER audio finishes
                    stopConversationMode();
                }
            };

            await audioElement.play();

        } catch (error) {
            console.error('TTS Error:', error);

            // Fallback to browser speech synthesis
            const utterance = new SpeechSynthesisUtterance(text);

            // Animate Three.js avatar for fallback speech too
            utterance.onstart = () => {
                startSpeaking();
            };

            utterance.onend = () => {
                stopSpeaking();
                isBotSpeaking = false;
                isProcessing = false; // Done processing/speaking

                // RESTART LISTENING HERE (Fallback)
                if (isConversationActive && !shouldEnd) {
                    statusDiv.textContent = "Listening... ðŸŸ¢";
                    statusDiv.style.color = "#42d392";

                    try {
                        recognition.start();
                    } catch (e) {
                        console.log('Already started');
                    }
                } else if (shouldEnd) {
                    stopConversationMode();
                }
            };

            window.speechSynthesis.speak(utterance);
        }
    }

    function base64ToBlob(base64, mimeType) {
        const byteCharacters = atob(base64);
        const byteNumbers = new Array(byteCharacters.length);

        for (let i = 0; i < byteCharacters.length; i++) { byteNumbers[i] = byteCharacters.charCodeAt(i); } const byteArray = new
            Uint8Array(byteNumbers); return new Blob([byteArray], { type: mimeType });
    } </script>
</body>

</html>