<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google ADK Voice Chatbot</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .container {
            background: rgba(255, 255, 255, 0.95);
            padding: 2rem;
            border-radius: 20px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
            text-align: center;
            max-width: 1000px;
            width: 90%;
            display: flex;
            flex-direction: column;
            max-height: 90vh;
            transition: all 0.3s ease;
        }

        .main-content {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            flex: 1;
            min-height: 0;
            width: 100%;
        }

        .left-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
        }

        .right-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            height: 100%;
            min-width: 300px;
        }

        h1 {
            color: #667eea;
            margin-bottom: 1.5rem;
            font-size: 2rem;
        }

        /* Three.js Avatar Container */
        .avatar-container {
            position: relative;
            width: 300px;
            height: 300px;
            margin: 0 auto 0 auto;
        }

        #avatar-canvas {
            width: 100%;
            height: 100%;
            border-radius: 20px;
            background: transparent;
        }

        #status {
            margin-bottom: 1rem;
            color: #666;
            font-style: italic;
            min-height: 1.5em;
            font-size: 0.95rem;
        }

        .mic-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 50%;
            width: 80px;
            height: 80px;
            font-size: 32px;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }

        .mic-btn:hover {
            transform: scale(1.05);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .mic-btn.listening {
            animation: pulse 1.5s infinite;
            background: linear-gradient(135deg, #ff6b6b 0%, #ee5253 100%);
            box-shadow: 0 4px 15px rgba(238, 82, 83, 0.4);
        }

        .action-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 30px;
            padding: 12px 30px;
            font-size: 1rem;
            cursor: pointer;
            transition: all 0.3s ease;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
            margin: 10px;
            font-weight: 600;
        }

        .action-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.6);
        }

        .action-btn:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
            box-shadow: none;
        }

        @keyframes pulse {
            0% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(238, 82, 83, 0.7);
            }

            70% {
                transform: scale(1.1);
                box-shadow: 0 0 0 10px rgba(238, 82, 83, 0);
            }

            100% {
                transform: scale(1);
                box-shadow: 0 0 0 0 rgba(238, 82, 83, 0);
            }
        }

        /* Chat Layout */
        #transcript {
            margin-top: 0;
            padding: 1rem;
            border-radius: 15px;
            background: #f8f9fa;
            min-height: 200px;
            max-height: 500px;
            overflow-y: auto;
            /* Flex layout for bubbles */
            display: flex;
            flex-direction: column;
            gap: 10px;

            text-align: left;
            box-shadow: inset 0 2px 4px rgba(0, 0, 0, 0.05);
            flex: 1;
            width: 100%;
            box-sizing: border-box;
        }

        .message {
            display: flex;
            flex-direction: column;
            max-width: 80%;
            animation: fadeIn 0.3s ease;
        }

        .user-message {
            align-self: flex-end;
            align-items: flex-end;
        }

        .bot-message {
            align-self: flex-start;
            align-items: flex-start;
        }

        .message-bubble {
            padding: 10px 15px;
            border-radius: 18px;
            position: relative;
            font-size: 0.95rem;
            line-height: 1.5;
            box-shadow: 0 1px 2px rgba(0, 0, 0, 0.1);
        }

        .user-message .message-bubble {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border-bottom-right-radius: 4px;
        }

        .bot-message .message-bubble {
            background: #e4e6eb;
            color: #050505;
            border-bottom-left-radius: 4px;
        }

        .sender-name {
            font-size: 0.75rem;
            margin-bottom: 4px;
            opacity: 0.8;
            margin-left: 10px;
            /* Slight indent */
        }

        .user-message .sender-name {
            margin-right: 10px;
            margin-left: 0;
            color: #667eea;
            text-align: right;
            display: none;
            /* Hide 'You' label for cleaner look, or set to block if preferred */
        }

        .bot-message .sender-name {
            color: #666;
            /* display: block; */
            display: none;
            /* Let's hide both for now for pure bubble look, unless user wants them. Code below puts them inside bubble? No, separate. */
        }

        /* Let's actually put the name INSIDE the bubble for "similar layout" request if we want */
        /* Re-reading request: "Layout 'You' and 'Bot' in a similar way" */
        /* I'll use a structure where name is bold inside the bubble for now, to replicate old feel but better */

        .message-content-inner {
            display: flex;
            flex-direction: column;
        }

        .bubble-sender {
            font-weight: bold;
            font-size: 0.8em;
            margin-bottom: 2px;
            opacity: 0.9;
        }

        @keyframes fadeIn {
            from {
                opacity: 0;
                transform: translateY(10px);
            }

            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        .hidden {
            display: none !important;
        }

        .subtitle {
            color: #666;
            margin-bottom: 2rem;
            font-size: 0.9rem;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            line-height: 1.6;
        }

        .interaction-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 80px;
            /* Fixed height taller than button */
            width: 100%;
            margin-top: 0;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            .main-content {
                flex-direction: column;
            }

            .right-panel {
                width: 100%;
                min-width: auto;
                min-height: 300px;
            }

            .container {
                padding: 1.5rem;
                height: auto;
                max-height: none;
            }
        }
    </style>
</head>

<div class="container">
    <h1>Ask Jack about the RFAM database</h1>
    <p class="subtitle">By Naveen Chawla. Work in progress - natural language voice conversation with a custom 3D
        avatar with an SQL database, Google Search capable, integrated with three prominent text-to-speech voice AI
        model systems</p>

    <div class="main-content">
        <div class="left-panel">
            <div class="controls" style="margin-bottom: 0.5rem; text-align: center;">
                <label for="ttsProvider" style="margin-right: 0.5rem; color: #333;">Select Voice Provider:</label>
                <select id="ttsProvider" style="padding: 0.5rem; border-radius: 8px; border: 1px solid #ccc;">
                    <option value="google">Google Chirp</option>
                    <option value="openai" selected>OpenAI (Default)</option>
                    <option value="elevenlabs">ElevenLabs (Pro)</option>
                </select>
            </div>

            <!-- Three.js 3D Avatar -->
            <!-- Three.js 3D Avatar -->
            <div class="avatar-container">
                <div id="avatar-loader"
                    style="position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%); color: #667eea; font-weight: bold; background: rgba(255,255,255,0.8); padding: 5px 10px; border-radius: 8px; pointer-events: none;">
                    Loading Avatar...</div>
                <canvas id="avatar-canvas"></canvas>
            </div>

            <div class="interaction-container">
                <div id="status" class="hidden"></div>
                <button id="startBtn" class="action-btn start-btn">Start Conversation</button>
            </div>
        </div>

        <div class="right-panel">
            <div id="transcript"></div>
        </div>
    </div>
</div>

<!-- Three.js Library -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
<!-- GLTFLoader -->
<script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>
<!-- Three-VRM -->
<script src="https://unpkg.com/@pixiv/three-vrm@0.6.11/lib/three-vrm.min.js"></script>

<!-- Three.js Avatar Setup -->
<script>
    // Three.js Scene Setup
    let scene,
        camera,
        renderer,
        clock;

    let currentVrm = null; // The VRM instance
    let isSpeaking = false;

    // Audio analysis
    let audioContext;
    let analyser;
    let dataArray;
    let currentAudioVolume = 0;
    let audioElement = null; // Debug fix: Global declaration

    // Blink State
    let blinkTimer = 0;
    let isBlinking = false;
    let blinkClose = false; // closing vs opening

    function initThreeJS() {
        const canvas = document.getElementById('avatar-canvas');
        const container = document.querySelector('.avatar-container');

        // Scene
        scene = new THREE.Scene();

        // Camera - Standard Portrait
        camera = new THREE.PerspectiveCamera(30, container.clientWidth / container.clientHeight, 0.1, 20.0);
        camera.position.set(0, 1.4, 1.2);
        camera.lookAt(0, 1.35, 0);

        // Renderer
        renderer = new THREE.WebGLRenderer({
            canvas: canvas,
            alpha: true,
            antialias: true
        });
        renderer.setSize(container.clientWidth, container.clientHeight);
        renderer.setClearColor(0x000000, 0);
        renderer.outputEncoding = THREE.sRGBEncoding;

        // Lights
        const light = new THREE.DirectionalLight(0xffffff);
        light.position.set(1.0, 1.0, 1.0).normalize();
        scene.add(light);

        const ambient = new THREE.AmbientLight(0xffffff, 0.8);
        scene.add(ambient);

        // Clock
        clock = new THREE.Clock();

        // Load Avatar
        loadAvatar();

        // Animation loop
        animate();
    }

    function loadAvatar() {
        const loader = new THREE.GLTFLoader();
        const avatarLoader = document.getElementById('avatar-loader');

        // Standard VRM file path
        const modelUrl = '/static/shadow.vrm';

        if (avatarLoader) {
            avatarLoader.classList.remove('hidden');
        }
        console.log("Loading VRM:", modelUrl);

        loader.crossOrigin = 'anonymous';
        loader.load(
            modelUrl,
            (gltf) => {
                THREE.VRM.from(gltf).then((vrm) => {
                    scene.add(vrm.scene);
                    currentVrm = vrm;

                    // Rotate 180 deg to face camera
                    vrm.scene.rotation.y = Math.PI;

                    if (avatarLoader) avatarLoader.classList.add('hidden');

                    // Adjust Camera to Face
                    const head = vrm.humanoid.getBoneNode(THREE.VRMSchema.HumanoidBoneName.Head);
                    if (head) {
                        const worldPos = new THREE.Vector3();
                        head.getWorldPosition(worldPos);

                        // Simple Camera Reset if head is found
                        // Assuming scale is roughly human (meters)
                        camera.position.set(0, worldPos.y, worldPos.z + 1.0);
                        camera.lookAt(0, worldPos.y, 0);
                    } else {
                        console.warn("Head bone not found, falling back to scene center.");
                        camera.position.set(0, 1.6, 1.5);
                        camera.lookAt(0, 1.0, 0);
                    }
                });
            },
            (progress) => { /* console.log... */ },
            (error) => {
                console.error("Failed to load VRM:", error);
                if (statusDiv) {
                    statusDiv.textContent = "Error loading avatar.vrm";
                    statusDiv.style.color = "red";
                }
            }
        );
    }

    function animate() {
        requestAnimationFrame(animate);

        const delta = clock.getDelta();

        if (currentVrm) {
            // 1. Blink Update
            updateBlink(delta);

            // 2. Lip Sync Update
            updateLipSync(delta);

            // 3. Update VRM
            currentVrm.update(delta);
        }

        renderer.render(scene, camera);
    }

    function updateBlink(delta) {
        // Support for VRM 0.x (blendShapeProxy) vs 1.0+ (expressionManager)
        const manager = currentVrm.blendShapeProxy || currentVrm.expressionManager;
        if (!manager) return;

        const BlinkPreset = currentVrm.blendShapeProxy
            ? THREE.VRMSchema.BlendShapePresetName.Blink
            : THREE.VRMSchema.ExpressionPresetName.Blink;

        blinkTimer -= delta;
        if (blinkTimer <= 0 && !isBlinking) {
            isBlinking = true;
            blinkClose = true;
            blinkTimer = Math.random() * 3.0 + 2.0;
        }

        if (isBlinking) {
            const blinkSpeed = 10.0;
            const currentVal = manager.getValue(BlinkPreset);

            if (blinkClose) {
                const newVal = Math.min(1.0, currentVal + delta * blinkSpeed);
                manager.setValue(BlinkPreset, newVal);
                if (newVal >= 1.0) blinkClose = false;
            } else {
                const newVal = Math.max(0.0, currentVal - delta * blinkSpeed);
                manager.setValue(BlinkPreset, newVal);
                if (newVal <= 0.0) isBlinking = false;
            }
        }
    }

    function updateLipSync(delta) {
        // Support for VRM 0.x (blendShapeProxy) vs 1.0+ (expressionManager)
        const manager = currentVrm.blendShapeProxy || currentVrm.expressionManager;
        if (!manager) return;

        // Determine Presets based on version
        const isV0 = !!currentVrm.blendShapeProxy;
        const SchemaStart = isV0 ? THREE.VRMSchema.BlendShapePresetName : THREE.VRMSchema.ExpressionPresetName;

        if (!isSpeaking) {
            const vowels = ['Aa', 'Ih', 'Ou', 'Ee', 'Oh']; // Capitalized for both usually works or is mapped
            vowels.forEach(v => {
                const preset = SchemaStart[v] || SchemaStart[v.toLowerCase()] || SchemaStart[v.toUpperCase()];
                if (preset) {
                    manager.setValue(preset, 0);
                }
            });
            return;
        }

        // Map Audio Volume directly to "Aa" or "A"
        const pName = SchemaStart.Aa || SchemaStart.A;
        if (!pName) return;

        const targetVal = Math.min(1.0, currentAudioVolume * 3.0);

        const currentVal = manager.getValue(pName);
        const newVal = THREE.MathUtils.lerp(currentVal, targetVal, 0.4);

        manager.setValue(pName, newVal);
    }

    // --- Shared Audio Control Functions ---
    function startSpeaking() { isSpeaking = true; }

    function stopSpeaking() {
        isSpeaking = false;
        currentAudioVolume = 0;
        if (currentVrm) {
            const manager = currentVrm.blendShapeProxy || currentVrm.expressionManager;
            if (manager) {
                const isV0 = !!currentVrm.blendShapeProxy;
                const SchemaStart = isV0 ? THREE.VRMSchema.BlendShapePresetName : THREE.VRMSchema.ExpressionPresetName;
                const pName = SchemaStart.Aa || SchemaStart.A;
                if (pName) manager.setValue(pName, 0);
            }
        }
    }

    function updateAudioVolume() {
        if (!isSpeaking || !analyser) return;
        analyser.getByteFrequencyData(dataArray);
        let sum = 0;
        for (let i = 0; i < dataArray.length; i++) sum += dataArray[i];
        currentAudioVolume = (sum / dataArray.length) / 255;

        if (isSpeaking) requestAnimationFrame(updateAudioVolume);
    }

    window.addEventListener('resize', () => {
        const container = document.querySelector('.avatar-container');
        camera.aspect = container.clientWidth / container.clientHeight;
        camera.updateProjectionMatrix();
        renderer.setSize(container.clientWidth, container.clientHeight);
    });

    window.addEventListener('load', initThreeJS);

</script>
<script>
    const startBtn = document.getElementById('startBtn');
    const statusDiv = document.getElementById('status');
    const transcriptDiv = document.getElementById('transcript');

    let sessionId = null;
    let recognition;
    let isConversationActive = false; // Flag for continuous mode
    let isBotSpeaking = false; // Flag to prevent self-triggering
    let isProcessing = false; // Flag to prevent restart during fetch

    if ('webkitSpeechRecognition' in window) {
        recognition = new webkitSpeechRecognition();
        recognition.continuous = false; // We manually restart
        recognition.interimResults = false;
        recognition.lang = 'en-US';

        recognition.onstart = () => {
            statusDiv.textContent = "Listening... ðŸŸ¢";
            statusDiv.style.color = "#42d392"; // Greenish
        };

        recognition.onend = () => {
            // If conversation is active and we are NOT processing a result and NOT speaking
            // Then it was just a silence timeout or no-speech event. Restart listening.
            if (isConversationActive && !isBotSpeaking && !isProcessing) {
                console.log('Recognition ended (silence), restarting loop...');

                // Don't change status text to "Processing", keep "Listening" (or similar)
                // to avoid confusing the user.
                try {
                    recognition.start();
                } catch (e) {
                    // Ignore if already started
                }
            } else if (!isConversationActive) {
                // Only show "Processing" if we are truly stopping/done?
                // Actually if we are not active, we are just stopped.
                statusDiv.textContent = "";
                statusDiv.style.color = "#666";
            }
            // If isProcessing is true, do NOTHING here. Let the fetch/speak completion handle the restart.
        };

        recognition.onresult = async (event) => {
            const transcript = event.results[0][0].transcript;
            addMessage('You', transcript);

            // Set processing flag to true so onend doesn't restart listing immediately
            isProcessing = true;

            // Visual feedback for processing
            statusDiv.textContent = "Thinking... ðŸ§ ";
            statusDiv.style.color = "#764ba2"; // Purple

            try {
                const requestBody = {
                    message: transcript
                };

                if (sessionId) {
                    requestBody.session_id = sessionId;
                }

                const response = await fetch('/chat', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify(requestBody)
                });


                const data = await response.json();
                console.log('Received data:', data);
                console.log('Response text:', data.response);

                if (data.session_id) {
                    sessionId = data.session_id;
                }

                let botResponse = data.response;
                console.log('Bot response (raw):', botResponse);

                // Check for termination token
                let shouldEndConversation = false;

                if (botResponse.includes('[END_CONVERSATION]')) {
                    shouldEndConversation = true;
                    botResponse = botResponse.replace('[END_CONVERSATION]', '').trim();
                }

                // --- EMOTION PARSING (SIMPLIFIED FOR VRM) ---
                // VRM usually handles emotions via Presets. 
                // We can map Emotions to VRM Presets here if desired.
                // For now, let's just strip the tag.
                const expressionRegex = /\[Expression:\s*([a-zA-Z]+)\]/i;
                const match = botResponse.match(expressionRegex);

                if (match && match[1]) {
                    let detectedEmotion = match[1];
                    console.log("Detected Emotion (Tag):", detectedEmotion);
                    // TODO: Map to VRM Expression if needed: currentVrm.expressionManager.setValue(detectedEmotion, 1.0);

                    // Remove tag from spoken text
                    botResponse = botResponse.replace(expressionRegex, '').trim();
                }

                addMessage('Bot', botResponse);

                // Respond
                await speak(botResponse, shouldEndConversation);

            } catch (error) {
                console.error('Error:', error);
                statusDiv.textContent = "Error communicating with server";
                isProcessing = false; // Reset if error

                // Force restart listening if error occurred
                if (isConversationActive) {
                    try {
                        recognition.start();
                    } catch (e) { }
                }
            }
        };


        recognition.onerror = (event) => {
            console.error('Speech recognition error', event.error);

            if (event.error === 'no-speech' && isConversationActive && !isBotSpeaking) {
                // Just restart if it timed out with no speech
                statusDiv.textContent = "Still listening...";
                // onend will likely handle the restart
            } else {
                statusDiv.textContent = "Error: " + event.error;
            }
        };
    } else {
        statusDiv.textContent = "Web Speech API not supported in this browser.";
        startBtn.disabled = true;
    }

    function startConversationMode() {
        isConversationActive = true;

        // UI Updates
        startBtn.classList.add('hidden');
        statusDiv.classList.remove('hidden'); // Show status
        statusDiv.textContent = "Starting...";

        // Start listening immediately
        try {
            recognition.start();
        } catch (e) {
            console.error("Could not start recognition:", e);
            statusDiv.textContent = "Error starting listening. Please try again.";
        }
    }

    function stopConversationMode() {
        isConversationActive = false;

        if (recognition) {
            recognition.stop();
        }

        if (audioElement) {
            audioElement.pause();
            audioElement.currentTime = 0;
        }

        if (window.speechSynthesis) {
            window.speechSynthesis.cancel();
        }

        stopSpeaking(); // Avatar animation stop

        // UI Updates
        startBtn.classList.remove('hidden');
        statusDiv.textContent = "";
        statusDiv.classList.add('hidden'); // Hide status
    }

    startBtn.addEventListener('click', startConversationMode);


    function addMessage(sender, text) {
        const div = document.createElement('div');
        const isUser = sender === 'You';
        div.className = 'message ' + (isUser ? 'user-message' : 'bot-message');

        // New Layout: Bubble with sender inside
        div.innerHTML = `
            <div class="message-bubble">
                <div class="bubble-sender">${sender}</div>
                <div>${text}</div>
            </div>
        `;

        const transcriptDiv = document.getElementById('transcript'); // Ensure transcriptDiv is defined in scope or use global
        const tDiv = document.getElementById('transcript');
        tDiv.appendChild(div);
        tDiv.scrollTop = tDiv.scrollHeight;
    }

    async function speak(text, shouldEnd = false) {
        isBotSpeaking = true;

        // Stop listening while speaking to prevent feedback loop
        if (recognition) {
            recognition.stop();
        }

        try {

            // Call the Google TTS endpoint
            const response = await fetch('/tts', {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json'
                },
                body: JSON.stringify({
                    text: text,
                    language_code: 'en-GB',
                    voice_name: 'en-GB-Chirp3-HD-Algenib',
                    provider: document.getElementById('ttsProvider').value
                })
            });

            const data = await response.json();

            // Convert base64 audio to blob and play
            const audioBlob = base64ToBlob(data.audio, 'audio/mp3');
            const audioUrl = URL.createObjectURL(audioBlob);

            // Setup Web Audio API and audio element (only once)
            if (!audioElement) {
                audioElement = new Audio();

                // Create AudioContext and analyser
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                const bufferLength = analyser.frequencyBinCount;
                dataArray = new Uint8Array(bufferLength);

                // Create source and connect (only once!)
                audioSource = audioContext.createMediaElementSource(audioElement);
                audioSource.connect(analyser);
                analyser.connect(audioContext.destination);
            }

            // Set new audio source and play
            audioElement.src = audioUrl;

            // Start Three.js avatar animation when audio starts playing
            audioElement.onplay = () => {
                startSpeaking();
                updateAudioVolume();
                statusDiv.textContent = "Speaking... ðŸ—£ï¸";
                statusDiv.style.color = "#667eea"; // Blue
            };

            // Stop Three.js avatar animation when audio ends
            audioElement.onended = () => {
                stopSpeaking();
                URL.revokeObjectURL(audioUrl);

                isBotSpeaking = false;
                isProcessing = false; // Done processing/speaking

                // RESTART LISTENING HERE if conversation is active AND not ending
                if (isConversationActive && !shouldEnd) {
                    statusDiv.textContent = "Listening... ðŸŸ¢";
                    statusDiv.style.color = "#42d392";

                    try {
                        recognition.start();
                    } catch (e) {
                        console.log('Already started');
                    }
                } else if (shouldEnd) {
                    // End conversation properly AFTER audio finishes
                    stopConversationMode();
                }
            };

            await audioElement.play();

        } catch (error) {
            console.error('TTS Error:', error);

            // Fallback to browser speech synthesis
            const utterance = new SpeechSynthesisUtterance(text);

            // Animate Three.js avatar for fallback speech too
            utterance.onstart = () => {
                startSpeaking();
            };

            utterance.onend = () => {
                stopSpeaking();
                isBotSpeaking = false;
                isProcessing = false; // Done processing/speaking

                // RESTART LISTENING HERE (Fallback)
                if (isConversationActive && !shouldEnd) {
                    statusDiv.textContent = "Listening... ðŸŸ¢";
                    statusDiv.style.color = "#42d392";

                    try {
                        recognition.start();
                    } catch (e) {
                        console.log('Already started');
                    }
                } else if (shouldEnd) {
                    stopConversationMode();
                }
            };

            window.speechSynthesis.speak(utterance);
        }
    }

    function base64ToBlob(base64, mimeType) {
        const byteCharacters = atob(base64);
        const byteNumbers = new Array(byteCharacters.length);

        for (let i = 0; i < byteCharacters.length; i++) { byteNumbers[i] = byteCharacters.charCodeAt(i); } const byteArray = new
            Uint8Array(byteNumbers); return new Blob([byteArray], { type: mimeType });
    } </script>
</body>

</html>