<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Google ADK Voice Chatbot</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100vh;
            margin: 0;
        }

        .container {
            background: rgba(255, 255, 255, 0.95);
            padding: 2rem;
            border-radius: 20px;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.2);
            backdrop-filter: blur(10px);
            text-align: center;
            max-width: 1000px;
            width: 90%;
            display: flex;
            flex-direction: column;
            max-height: 90vh;
            transition: all 0.3s ease;
        }

        .main-content {
            display: flex;
            gap: 2rem;
            align-items: flex-start;
            flex: 1;
            min-height: 0;
            width: 100%;
        }

        .left-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 100%;
        }

        .right-panel {
            flex: 1;
            display: flex;
            flex-direction: column;
            height: 100%;
            min-width: 300px;
            /* No justify-center needed for text only */
        }

        /* Removed .right-panel.chat-active styles */

        h1 {
            color: #667eea;
            margin-bottom: 1.5rem;
            font-size: 2rem;
        }

        /* Three.js Avatar Container */
        .avatar-container {
            position: relative;
            width: 300px;
            height: 300px;
            margin: 0 auto 1.5rem auto;
        }

        #avatar-canvas {
            width: 100%;
            height: 100%;
            border-radius: 20px;
            background: transparent;
        }

        #status {
            margin-bottom: 1rem;
            color: #666;
            font-style: italic;
            min-height: 1.5em;
            font-size: 0.95rem;
        }

        .mic-btn {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 50%;
            width: 80px;
            height: 80px;
            font-size: 32px;
            cursor: pointer;
            transition: all 0.3s ease;
            display: flex;
            align-items: center;
            justify-content: center;
            margin: 0 auto;
            box-shadow: 0 4px 15px rgba(102, 126, 234, 0.4);
        }

        .mic-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(102, 126, 234, 0.5);
        }

        .mic-btn:active {
            transform: scale(0.95);
        }

        /* Start/Stop Button Styles */
        .action-btn {
            font-size: 1.2rem;
            padding: 1rem 2rem;
            border: none;
            border-radius: 50px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: bold;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
            text-transform: uppercase;
            letter-spacing: 1px;
            margin: 0 auto;
            width: 80%;
            max-width: 300px;
        }

        .start-btn {
            background: linear-gradient(135deg, #42d392 0%, #647eff 100%);
            color: white;
        }

        .start-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(66, 211, 146, 0.4);
        }

        .stop-btn {
            background: linear-gradient(135deg, #ff416c 0%, #ff4b2b 100%);
            color: white;
        }

        .stop-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(255, 65, 108, 0.4);
        }

        .hidden {
            display: none;
        }

        #transcript {
            margin-top: 0;
            padding: 1rem;
            background: rgba(248, 249, 250, 0.8);
            border-radius: 12px;
            text-align: left;
            flex: 1;
            min-height: 0;
            overflow-y: auto;
            backdrop-filter: blur(5px);
            width: 100%;
            flex: 1;
            /* Take remaining height in right panel */
            box-sizing: border-box;
            max-height: 600px;
        }

        .message {
            margin-bottom: 0.5rem;
            padding: 0.5rem;
            border-radius: 8px;
            transition: background 0.2s ease;
        }

        .message:hover {
            background: rgba(102, 126, 234, 0.1);
        }

        .user {
            color: #667eea;
            font-weight: bold;
        }

        .bot {
            color: #333;
        }

        /* Mobile Responsiveness */
        @media (max-width: 768px) {
            .main-content {
                flex-direction: column;
                align-items: center;
            }

            .left-panel,
            .right-panel {
                width: 100%;
                min-width: unset;
            }

            .right-panel {
                height: 300px;
                /* Fixed height for chat on mobile */
                flex: none;
            }

            #transcript {
                margin-top: 1rem;
            }
        }

        .interaction-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            height: 80px;
            /* Fixed height taller than button */
            width: 100%;
            margin-top: 1rem;
        }

        #status {
            font-size: 1.1rem;
            font-weight: 500;
            color: #666;
            text-align: center;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 0.95rem;
            margin-top: -1rem;
            margin-bottom: 2rem;
            max-width: 800px;
            margin-left: auto;
            margin-right: auto;
            line-height: 1.5;
            font-style: italic;
        }
    </style>
</head>

<body>
    <div class="container">
        <h1>Ask Jack about the RFAM database</h1>
        <p class="subtitle">By Naveen Chawla. Work in progress - natural language voice conversation with a custom 3D
            avatar with an SQL database, Google Search capable, integrated with three prominent text-to-speech voice AI
            model systems</p>

        <div class="main-content">
            <div class="left-panel">
                <div class="controls" style="margin-bottom: 0.5rem; text-align: center;">
                    <label for="ttsProvider" style="margin-right: 0.5rem; color: #333;">Select Voice Provider:</label>
                    <select id="ttsProvider" style="padding: 0.5rem; border-radius: 8px; border: 1px solid #ccc;">
                        <option value="google">Google Chirp (Default)</option>
                        <option value="openai">OpenAI (Pro)</option>
                        <option value="elevenlabs">ElevenLabs (Pro)</option>
                    </select>
                </div>

                <!-- Three.js 3D Avatar -->
                <div class="avatar-container">
                    <canvas id="avatar-canvas"></canvas>
                </div>

                <div class="interaction-container">
                    <div id="status" class="hidden"></div>
                    <button id="startBtn" class="action-btn start-btn">Start Conversation</button>
                </div>
            </div>

            <div class="right-panel">
                <div id="transcript"></div>
            </div>
        </div>
    </div>

    <!-- Three.js Library -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <!-- GLTFLoader for loading 3D models -->
    <script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"></script>

    <!-- Three.js Avatar Setup -->
    <script>
        // Three.js Scene Setup
        let scene,
            camera,
            renderer,
            avatar,
            mixer,
            clock;
        let isSpeaking = false;
        let animationId;
        let headBone,
            jawBone;

        // Audio analysis for mouth movement
        let audioContext;
        let analyser;
        let dataArray;
        let currentAudioVolume = 0;
        let audioElement; // Reusable audio element
        let audioSource; // Reusable audio source

        // Expression System
        let currentEmotion = "Neutral";
        let blinkInterval;
        let isBlinking = false;

        // Map Emotions to ARKit Blend Shapes (approximate for RPM)
        const expressionMap = {
            "Neutral": {},
            "Happy": {
                "mouthSmile": 0.7,
                "browInnerUp": 0.4,
                "cheekPuff": 0.2,
                "eyeSquintLeft": 0.3,
                "eyeSquintRight": 0.3
            },
            "Sad": {
                "mouthFrownLeft": 0.5,
                "mouthFrownRight": 0.5,
                "browDownLeft": 0.4,
                "browDownRight": 0.4,
                "eyeLookDownLeft": 0.2,
                "eyeLookDownRight": 0.2
            },
            "Surprised": {
                "eyeWideLeft": 0.6,
                "eyeWideRight": 0.6,
                "jawOpen": 0.1,
                "browInnerUp": 0.6,
                "browOuterUpLeft": 0.5,
                "browOuterUpRight": 0.5
            },
            "Thinking": {
                "browDownLeft": 0.3,
                "browInnerUp": 0.3,
                "mouthPucker": 0.3,
                "eyeLookUpRight": 0.4,
                "eyeLookUpLeft": 0.4
            },
            "Angry": {
                "browDownLeft": 0.8,
                "browDownRight": 0.8,
                "mouthShrugLower": 0.4,
                "eyeSquintLeft": 0.5,
                "eyeSquintRight": 0.5
            },
            "Confused": {
                "browDownLeft": 0.5,
                "browOuterUpRight": 0.5,
                "mouthRollLower": 0.3,
                "eyeLookInLeft": 0.3
            }
        };

        function initThreeJS() {
            const canvas = document.getElementById('avatar-canvas');
            const container = document.querySelector('.avatar-container');

            // Scene
            scene = new THREE.Scene();

            // Camera - positioned for close-up face view like a video call
            camera = new THREE.PerspectiveCamera(40, // Slightly wider field of view for better framing
                container.clientWidth / container.clientHeight,
                0.1,
                1000);
            camera.position.set(0, 0, 0.52); // Closer but not too close - video call distance
            camera.lookAt(0, 0, 0); // Look at face level

            // Renderer
            renderer = new THREE.WebGLRenderer({
                canvas: canvas,
                alpha: true,
                antialias: true
            });
            renderer.setSize(container.clientWidth, container.clientHeight);
            renderer.setClearColor(0x000000, 0);
            renderer.outputEncoding = THREE.sRGBEncoding;

            // Lights
            const ambientLight = new THREE.AmbientLight(0xffffff, 0.8);
            scene.add(ambientLight);

            const directionalLight = new THREE.DirectionalLight(0xffffff, 1.0);
            directionalLight.position.set(1, 2, 3);
            scene.add(directionalLight);

            const fillLight = new THREE.DirectionalLight(0xffffff, 0.5);
            fillLight.position.set(-1, 0, -1);
            scene.add(fillLight);

            // Clock for animations
            clock = new THREE.Clock();

            // Load Avatar
            loadAvatar();

            // Animation loop
            animate();

            // Start Blinking Engine
            startBlinking();
        }

        function startBlinking() {
            // Random blink interval between 3 and 6 seconds
            const nextBlink = Math.random() * 3000 + 3000;
            setTimeout(() => {
                triggerBlink();
                startBlinking();
            }, nextBlink);
        }

        function triggerBlink() {
            if (!window.morphTargetMesh) return;

            isBlinking = true;
            // Blink duration approx 150ms
            setTimeout(() => {
                isBlinking = false;
            }, 150);
        }

        function loadAvatar() {
            const loader = new THREE.GLTFLoader();

            // Using a verified Ready Player Me avatar URL
            // This is a full-body realistic human model
            const modelUrl = //'https://models.readyplayer.me/64bfa15f0e72c63d7c3934a6.glb'
                'https://models.readyplayer.me/6921e87fbcfe438b18908217.glb'
                ;

            console.log('Starting to load 3D model from:', modelUrl);
            statusDiv.textContent = "Loading 3D avatar...";

            loader.load(modelUrl,
                function (gltf) {
                    console.log('Model loaded successfully!', gltf);
                    avatar = gltf.scene;

                    // Calculate and log model height before adding to scene
                    const box = new THREE.Box3().setFromObject(avatar);
                    const size = new THREE.Vector3();
                    box.getSize(size);
                    console.log('Avatar Height (before scene add):', size.y);
                    console.log('Avatar Dimensions (before scene add):', size);

                    // Scale and position the model for close-up face view
                    avatar.scale.set(1, 1, 1);
                    avatar.position.set(0, -size.y + 0.15, 0); // Slightly lower to center face in view

                    scene.add(avatar);

                    console.log('Avatar added to scene');
                    statusDiv.textContent = "";

                    // Setup animations if available
                    if (gltf.animations && gltf.animations.length > 0) {
                        mixer = new THREE.AnimationMixer(avatar);
                        console.log('Animation mixer created with', gltf.animations.length, 'animations');
                    }

                    // Try to find head and jaw bones for animation
                    let boneCount = 0;
                    let morphTargetMesh = null;
                    let mouthOpenIndex = -1;
                    let jawOpenIndex = -1;

                    avatar.traverse((child) => {
                        if (child.isBone) {
                            boneCount++;
                            const boneName = child.name.toLowerCase();
                            console.log('Found bone:', child.name);

                            // Look for head bone (various naming conventions)
                            if (boneName.includes('head') || boneName.includes('neck_01')) {
                                headBone = child;
                                console.log('âœ“ Head bone found:', child.name);
                            }

                            // Look for jaw bone (various naming conventions)
                            if (boneName.includes('jaw') || boneName.includes('chin') || boneName.includes('mouth') || boneName === 'cc_base_jawroot' || boneName === 'mixamorig:jaw') {
                                jawBone = child;
                                console.log('âœ“ Jaw bone found:', child.name);
                            }
                        }

                        // Check for morph targets (blend shapes) for facial animation
                        if (child.isMesh && child.morphTargetDictionary && child.morphTargetInfluences) {
                            const morphNames = Object.keys(child.morphTargetDictionary);
                            console.log('âœ“ Morph targets found on mesh:', child.name);
                            console.log('  Available morphs:', morphNames);

                            // Look for mouth-related morph targets
                            morphNames.forEach(name => {
                                const lowerName = name.toLowerCase();

                                if (lowerName.includes('mouthopen') || lowerName.includes('mouth_open')) {
                                    mouthOpenIndex = child.morphTargetDictionary[name];
                                    morphTargetMesh = child;
                                    console.log('  âœ“ Found mouthOpen morph:', name, 'at index', mouthOpenIndex);
                                }

                                if (lowerName.includes('jawopen') || lowerName.includes('jaw_open')) {
                                    jawOpenIndex = child.morphTargetDictionary[name];
                                    morphTargetMesh = child;
                                    console.log('  âœ“ Found jawOpen morph:', name, 'at index', jawOpenIndex);
                                }
                            });
                        }
                    });

                    // Store morph target info globally for animation
                    if (morphTargetMesh) {
                        window.morphTargetMesh = morphTargetMesh;
                        window.mouthOpenIndex = mouthOpenIndex;
                        window.jawOpenIndex = jawOpenIndex;
                    }

                    console.log('Total bones found:', boneCount);
                    console.log('Head bone:', headBone ? headBone.name : 'NOT FOUND');
                    console.log('Jaw bone:', jawBone ? jawBone.name : 'NOT FOUND');
                    console.log('Morph target mesh:', morphTargetMesh ? 'FOUND' : 'NOT FOUND');

                    console.log('Mouth morph indices:', {
                        mouthOpen: mouthOpenIndex, jawOpen: jawOpenIndex
                    });
                    console.log('Avatar loaded successfully - this is a realistic human model!');
                }

                ,
                function (xhr) {
                    const percentComplete = (xhr.loaded / xhr.total * 100).toFixed(0);
                    console.log(percentComplete + '% loaded');

                    statusDiv.textContent = `Loading avatar: $ {
                    percentComplete
                }

                %`;
                }

                ,
                function (error) {
                    console.error('Error loading avatar:', error);
                    console.error('Error details:', error.message);
                    statusDiv.textContent = "Error loading avatar, using fallback";
                    // Fallback: create a simple placeholder
                    createFallbackAvatar();
                });
        }

        function createFallbackAvatar() {
            // Simple fallback if model fails to load
            avatar = new THREE.Group();

            const headGeometry = new THREE.SphereGeometry(0.3, 32, 32);

            const headMaterial = new THREE.MeshPhongMaterial({
                color: 0xffd4b3
            });
            const head = new THREE.Mesh(headGeometry, headMaterial);
            head.position.y = 1.6;
            avatar.add(head);

            const bodyGeometry = new THREE.CylinderGeometry(0.3, 0.4, 0.8, 16);

            const bodyMaterial = new THREE.MeshPhongMaterial({
                color: 0x667eea
            });
            const body = new THREE.Mesh(bodyGeometry, bodyMaterial);
            body.position.y = 0.8;
            avatar.add(body);

            scene.add(avatar);
            console.log('Using fallback avatar');
        }

        function animate() {
            animationId = requestAnimationFrame(animate);

            // Update animation mixer if it exists
            if (mixer) {
                const delta = clock.getDelta();
                mixer.update(delta);
            }

            // --- ANIMATION CORE ---
            if (avatar && window.morphTargetMesh) {
                const mesh = window.morphTargetMesh;
                const time = Date.now() * 0.001;

                // 1. Reset all known morphs to 0 first (clean slate)
                // In a robust engine we'd only reset the ones we use, but iterating dictionary keys is safer for now
                // Optimization: getting keys every frame is bad, usually we'd cache indices.
                // For now, let's just reset the ones in our Expression Map + Lip Sync specific ones

                // Helper to safely set weight
                const setWeight = (name, val) => {
                    const idx = mesh.morphTargetDictionary[name];
                    if (idx !== undefined) {
                        mesh.morphTargetInfluences[idx] = val;
                    }
                };

                // Reset common expression morphs (brute force clear of core set)
                // A better way is to iterate all expressionMap keys and reset them.
                // Or just assume 'Neutral' is 0.
                // Let's iterate available morphs and zero them? No, too expensive.
                // Let's just Apply Tone -> Apply Lip Sync -> Apply Blink.

                // Actually, blend shapes are additive usually. We need to clear previous frame's influence 
                // if we want to change expressions cleanly.
                // Simpler approach: We just set the target values every frame.

                // Default: Reset everything to 0? 
                // RPM avatars have many morphs. Resetting 52 every frame is fine in JS (it's just an array).
                for (let i = 0; i < mesh.morphTargetInfluences.length; i++) {
                    mesh.morphTargetInfluences[i] = THREE.MathUtils.lerp(mesh.morphTargetInfluences[i], 0, 0.1); // Smooth return to neutral
                }

                // 2. Apply Base Expression
                if (currentEmotion && expressionMap[currentEmotion]) {
                    const targetShapes = expressionMap[currentEmotion];
                    for (const [shapeName, targetCtx] of Object.entries(targetShapes)) {
                        const idx = mesh.morphTargetDictionary[shapeName];
                        if (idx !== undefined) {
                            // Smoothly interpolate to target weight
                            const current = mesh.morphTargetInfluences[idx];
                            mesh.morphTargetInfluences[idx] = THREE.MathUtils.lerp(current, targetCtx, 0.1);
                        }
                    }
                }

                // 3. Apply Lip Sync (Mouth Movement) overrides
                if (isSpeaking) {
                    const mouthValue = currentAudioVolume * 2.5;

                    if (window.mouthOpenIndex >= 0) {
                        // Additive? Or Override?
                        // If we are smiling, opening mouth is additive.
                        // But jawOpen might conflict.
                        const current = mesh.morphTargetInfluences[window.mouthOpenIndex];
                        // Smoothly open
                        mesh.morphTargetInfluences[window.mouthOpenIndex] = THREE.MathUtils.lerp(current, mouthValue, 0.2);
                    }
                    if (window.jawOpenIndex >= 0) {
                        const current = mesh.morphTargetInfluences[window.jawOpenIndex];
                        mesh.morphTargetInfluences[window.jawOpenIndex] = THREE.MathUtils.lerp(current, mouthValue * 0.3, 0.2);
                    }
                }

                // 4. Apply Blinking (High Priority)
                if (isBlinking) {
                    setWeight('eyeBlinkLeft', 1.0);
                    setWeight('eyeBlinkRight', 1.0);
                }

                // 5. Idle Head Movement (Procedural)
                if (headBone) {
                    headBone.rotation.y = Math.sin(time * 0.5) * 0.05; // Look left/right slowly
                    headBone.rotation.x = Math.sin(time * 0.3) * 0.02; // Nod slightly
                }
            }
            else if (avatar && !window.morphTargetMesh) {
                // Fallback animation for non-morph avatars (Spheres etc)
                const time = Date.now() * 0.001;
                if (isSpeaking) {
                    avatar.position.y = Math.sin(time * 10) * 0.02;
                    avatar.scale.y = 1 + Math.sin(time * 20) * 0.05;
                } else {
                    avatar.position.y = Math.sin(time) * 0.02;
                    avatar.scale.y = 1;
                }
            }

            renderer.render(scene, camera);
        }

        // Start speaking animation
        function startSpeaking() {
            isSpeaking = true;
        }

        // Stop speaking animation
        function stopSpeaking() {
            isSpeaking = false;
            currentAudioVolume = 0; // Reset volume

            // Reset expression to neutral when done speaking (optional, or keep it lingering?)
            // Let's linger on the expression for a second then fade?
            // For now, let's keep it until next turn or reset after a delay.
            // Actually, resetting to Neutral immediately feels robotic. Let's start a timeout to reset.
            setTimeout(() => {
                if (!isSpeaking) currentEmotion = "Neutral";
            }, 2000);
        }

        // Update audio volume from analyser
        function updateAudioVolume() {
            if (!isSpeaking || !analyser) return;

            analyser.getByteFrequencyData(dataArray);

            // Calculate average volume
            let sum = 0;

            for (let i = 0; i < dataArray.length; i++) {
                sum += dataArray[i];
            }

            const average = sum / dataArray.length;

            // Normalize to 0-1 range and smooth it
            currentAudioVolume = average / 255;

            // Continue updating while speaking
            if (isSpeaking) {
                requestAnimationFrame(updateAudioVolume);
            }
        }

        // Handle window resize
        window.addEventListener('resize', () => {
            const container = document.querySelector('.avatar-container');
            camera.aspect = container.clientWidth / container.clientHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(container.clientWidth, container.clientHeight);
        });

        // Initialize Three.js when page loads
        window.addEventListener('load', initThreeJS);
    </script>
    <script>const startBtn = document.getElementById('startBtn');
        const statusDiv = document.getElementById('status');
        const transcriptDiv = document.getElementById('transcript');

        let sessionId = null;
        let recognition;
        let isConversationActive = false; // Flag for continuous mode
        let isBotSpeaking = false; // Flag to prevent self-triggering
        let isProcessing = false; // Flag to prevent restart during fetch

        if ('webkitSpeechRecognition' in window) {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = false; // We manually restart
            recognition.interimResults = false;
            recognition.lang = 'en-US';

            recognition.onstart = () => {
                statusDiv.textContent = "Listening... ðŸŸ¢";
                statusDiv.style.color = "#42d392"; // Greenish
            }

                ;

            recognition.onend = () => {

                // If conversation is active and we are NOT processing a result and NOT speaking
                // Then it was just a silence timeout or no-speech event. Restart listening.
                if (isConversationActive && !isBotSpeaking && !isProcessing) {
                    console.log('Recognition ended (silence), restarting loop...');

                    // Don't change status text to "Processing", keep "Listening" (or similar) 
                    // to avoid confusing the user.
                    try {
                        recognition.start();
                    }

                    catch (e) {
                        // Ignore if already started
                    }
                }

                else if (!isConversationActive) {
                    // Only show "Processing" if we are truly stopping/done? 
                    // Actually if we are not active, we are just stopped.
                    statusDiv.textContent = "";
                    statusDiv.style.color = "#666";
                }

                // If isProcessing is true, do NOTHING here. Let the fetch/speak completion handle the restart.
            }

                ;

            recognition.onresult = async (event) => {
                const transcript = event.results[0][0].transcript;
                addMessage('You', transcript);

                // Set processing flag to true so onend doesn't restart listing immediately
                isProcessing = true;

                // Visual feedback for processing
                statusDiv.textContent = "Thinking... ðŸ§ ";
                statusDiv.style.color = "#764ba2"; // Purple

                try {
                    const requestBody = {
                        message: transcript
                    }

                        ;

                    if (sessionId) {
                        requestBody.session_id = sessionId;
                    }

                    const response = await fetch('/chat', {

                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        }

                        ,
                        body: JSON.stringify(requestBody)
                    });


                    const data = await response.json();
                    console.log('Received data:', data);
                    console.log('Response text:', data.response);

                    if (data.session_id) {
                        sessionId = data.session_id;
                    }

                    let botResponse = data.response;
                    console.log('Bot response (raw):', botResponse);

                    // Check for termination token
                    let shouldEndConversation = false;

                    if (botResponse.includes('[END_CONVERSATION]')) {
                        shouldEndConversation = true;
                        botResponse = botResponse.replace('[END_CONVERSATION]', '').trim();
                    }

                    // --- EMOTION PARSING ---
                    // Look for [Expression: X] tag
                    const expressionRegex = /\[Expression:\s*([a-zA-Z]+)\]/i;
                    const match = botResponse.match(expressionRegex);

                    if (match && match[1]) {
                        // Found an emotion tag!
                        let detectedEmotion = match[1];
                        // Normalize case (e.g. "happy" -> "Happy")
                        detectedEmotion = detectedEmotion.charAt(0).toUpperCase() + detectedEmotion.slice(1).toLowerCase();

                        console.log("Detected Emotion:", detectedEmotion);

                        if (expressionMap[detectedEmotion]) {
                            currentEmotion = detectedEmotion;
                        } else {
                            console.log("Unknown emotion:", detectedEmotion, "defaulting to Neutral");
                            currentEmotion = "Neutral";
                        }

                        // Remove tag from spoken text so we don't say "Expression Happy"
                        botResponse = botResponse.replace(expressionRegex, '').trim();
                    } else {
                        // Default fallback if no tag found (though system prompt should enforce it)
                        currentEmotion = "Neutral";
                    }

                    addMessage('Bot', botResponse);

                    // Respond
                    await speak(botResponse, shouldEndConversation);

                    // Do NOT call stopConversationMode() here immediately.
                    // It will be handled in speak() -> onended callback.

                }

                catch (error) {
                    console.error('Error:', error);
                    statusDiv.textContent = "Error communicating with server";
                    isProcessing = false; // Reset if error

                    // Force restart listening if error occurred
                    if (isConversationActive) {
                        try {
                            recognition.start();
                        }

                        catch (e) { }
                    }
                }
            }

                ;


            recognition.onerror = (event) => {
                console.error('Speech recognition error', event.error);

                if (event.error === 'no-speech' && isConversationActive && !isBotSpeaking) {
                    // Just restart if it timed out with no speech
                    statusDiv.textContent = "Still listening...";
                    // onend will likely handle the restart
                }

                else {
                    statusDiv.textContent = "Error: " + event.error;
                }
            }

                ;
        }

        else {
            statusDiv.textContent = "Web Speech API not supported in this browser.";
            startBtn.disabled = true;
        }

        function startConversationMode() {
            isConversationActive = true;

            // UI Updates
            startBtn.classList.add('hidden');
            statusDiv.classList.remove('hidden'); // Show status
            statusDiv.textContent = "Starting...";

            // Start listening immediately
            try {
                recognition.start();
            }

            catch (e) {
                console.error("Could not start recognition:", e);
                statusDiv.textContent = "Error starting listening. Please try again.";
            }
        }

        function stopConversationMode() {
            isConversationActive = false;

            if (recognition) {
                recognition.stop();
            }

            if (audioElement) {
                audioElement.pause();
                audioElement.currentTime = 0;
            }

            if (window.speechSynthesis) {
                window.speechSynthesis.cancel();
            }

            stopSpeaking(); // Avatar animation stop

            // UI Updates
            startBtn.classList.remove('hidden');
            statusDiv.textContent = "";
            statusDiv.classList.add('hidden'); // Hide status
        }

        startBtn.addEventListener('click', startConversationMode);


        function addMessage(sender, text) {
            const div = document.createElement('div');
            div.className = 'message';

            div.innerHTML = `<span class="${sender.toLowerCase()}">${sender}:</span> ${text}`;
            transcriptDiv.appendChild(div);
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        async function speak(text, shouldEnd = false) {
            isBotSpeaking = true;

            // Stop listening while speaking to prevent feedback loop
            if (recognition) {
                recognition.stop();
            }

            try {

                // Call the Google TTS endpoint
                const response = await fetch('/tts', {

                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    }

                    ,
                    body: JSON.stringify({
                        text: text,
                        text: text,
                        language_code: 'en-GB',
                        voice_name: 'en-GB-Chirp3-HD-Algenib',
                        provider: document.getElementById('ttsProvider').value
                    })
                });

                const data = await response.json();

                // Convert base64 audio to blob and play
                const audioBlob = base64ToBlob(data.audio, 'audio/mp3');
                const audioUrl = URL.createObjectURL(audioBlob);

                // Setup Web Audio API and audio element (only once)
                if (!audioElement) {
                    audioElement = new Audio();

                    // Create AudioContext and analyser
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    analyser = audioContext.createAnalyser();
                    analyser.fftSize = 256;
                    const bufferLength = analyser.frequencyBinCount;
                    dataArray = new Uint8Array(bufferLength);

                    // Create source and connect (only once!)
                    audioSource = audioContext.createMediaElementSource(audioElement);
                    audioSource.connect(analyser);
                    analyser.connect(audioContext.destination);
                }

                // Set new audio source and play
                audioElement.src = audioUrl;

                // Start Three.js avatar animation when audio starts playing
                audioElement.onplay = () => {
                    startSpeaking();
                    updateAudioVolume();
                    statusDiv.textContent = "Speaking... ðŸ—£ï¸";
                    statusDiv.style.color = "#667eea"; // Blue
                }

                    ;

                // Stop Three.js avatar animation when audio ends
                audioElement.onended = () => {
                    stopSpeaking();
                    URL.revokeObjectURL(audioUrl);

                    isBotSpeaking = false;
                    isProcessing = false; // Done processing/speaking

                    // RESTART LISTENING HERE if conversation is active AND not ending
                    if (isConversationActive && !shouldEnd) {
                        statusDiv.textContent = "Listening... ðŸŸ¢";
                        statusDiv.style.color = "#42d392";

                        try {
                            recognition.start();
                        }

                        catch (e) {
                            console.log('Already started');
                        }
                    }

                    else if (shouldEnd) {
                        // End conversation properly AFTER audio finishes
                        stopConversationMode();
                    }
                }

                    ;

                await audioElement.play();

            }

            catch (error) {
                console.error('TTS Error:', error);

                // Fallback to browser speech synthesis
                const utterance = new SpeechSynthesisUtterance(text);

                // Animate Three.js avatar for fallback speech too
                utterance.onstart = () => {
                    startSpeaking();
                }

                    ;

                utterance.onend = () => {
                    stopSpeaking();
                    isBotSpeaking = false;
                    isProcessing = false; // Done processing/speaking

                    // RESTART LISTENING HERE (Fallback)
                    if (isConversationActive && !shouldEnd) {
                        statusDiv.textContent = "Listening... ðŸŸ¢";
                        statusDiv.style.color = "#42d392";

                        try {
                            recognition.start();
                        }

                        catch (e) {
                            console.log('Already started');
                        }
                    }

                    else if (shouldEnd) {
                        stopConversationMode();
                    }
                }

                    ;

                window.speechSynthesis.speak(utterance);
            }
        }

        function base64ToBlob(base64, mimeType) {
            const byteCharacters = atob(base64);
            const byteNumbers = new Array(byteCharacters.length);

            for (let i = 0; i < byteCharacters.length; i++) {
                byteNumbers[i] = byteCharacters.charCodeAt(i);
            }

            const byteArray = new Uint8Array(byteNumbers);

            return new Blob([byteArray], {
                type: mimeType
            });
        }

    </script>
</body>

</html>